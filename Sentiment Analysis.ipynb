{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Download the Foursquare annotated comments in Brazilian Portuguese: https://www.kaggle.com/thaisalmeida/tips-foursquare/version/1\n",
    "\n",
    "Place the files in subfolder 'docs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget files if using Google Colab\n",
    "!wget -q https://raw.githubusercontent.com/douglas125/TextClassification/master/preProcessing.py\n",
    "!wget -q https://raw.githubusercontent.com/douglas125/TextClassification/master/Embeddings.py\n",
    "!wget -q https://raw.githubusercontent.com/douglas125/TextClassification/master/requirements.txt\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "\n",
    "#move CSVs to docs/ folder\n",
    "from google.colab import files\n",
    "files.upload()\n",
    "\n",
    "!mkdir docs\n",
    "!mv *.csv docs/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import preProcessing\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "pd.set_option('max_colwidth',150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>rotulo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A comida é deliciosa, mas pedi limonada suiça e me disseram que hoje estavam todos muito ocupados e que ninguém conseguiria me atender....melhor i...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A partir desta sexta feira dia 11 começam a abrir para jantar mas corre pois é só até as 22 hrs e no domingo dia das mães, estarão aberto durante ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joint burguer e brewdog</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agora de segunda a sexta o Habanero vai abrir no almoço com pratos mexicanos e tradicionais!</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Experimente o drink \"Dona Diabla\". Muito bom!</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nova senha do Wifi: 1129508219</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wi-fi 1129508219</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Adoramos a pizza carbonara e a paulistana. Não surpreendeu tanto, mas vale a pena por resgatar o tradicionalismo. Dica @Gourmet_For</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O diferencial desse Burger King é que você mesmo serve o refrigerante, e a vontade!</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Unico defeito estacionamento pago!</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24h é 24h.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Excelente Burger King: bom atendimento, ambiente agradável e refil para refrigerante. Evite o inconveniente estacionamento pago estacionando ao re...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>o atendimento aqui é simplesmente um lixo</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>O Drive-Truh mais lento da cidade!!</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cheio de bêbados depois das 2 da manhã :D</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Recinto sujo, sem manutenção. Não há segurança pra conter a briga que ocorreu agora há pouco. O refrigerante está aguado, sem gosto, horrível. O l...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    texto  \\\n",
       "0   A comida é deliciosa, mas pedi limonada suiça e me disseram que hoje estavam todos muito ocupados e que ninguém conseguiria me atender....melhor i...   \n",
       "1   A partir desta sexta feira dia 11 começam a abrir para jantar mas corre pois é só até as 22 hrs e no domingo dia das mães, estarão aberto durante ...   \n",
       "2                                                                                                                                 Joint burguer e brewdog   \n",
       "3                                                            Agora de segunda a sexta o Habanero vai abrir no almoço com pratos mexicanos e tradicionais!   \n",
       "4                                                                                                           Experimente o drink \"Dona Diabla\". Muito bom!   \n",
       "5                                                                                                                          Nova senha do Wifi: 1129508219   \n",
       "6                                                                                                                                        Wi-fi 1129508219   \n",
       "7                     Adoramos a pizza carbonara e a paulistana. Não surpreendeu tanto, mas vale a pena por resgatar o tradicionalismo. Dica @Gourmet_For   \n",
       "8                                                                     O diferencial desse Burger King é que você mesmo serve o refrigerante, e a vontade!   \n",
       "9                                                                                                                      Unico defeito estacionamento pago!   \n",
       "10                                                                                                                                             24h é 24h.   \n",
       "11  Excelente Burger King: bom atendimento, ambiente agradável e refil para refrigerante. Evite o inconveniente estacionamento pago estacionando ao re...   \n",
       "12                                                                                                              o atendimento aqui é simplesmente um lixo   \n",
       "13                                                                                                                    O Drive-Truh mais lento da cidade!!   \n",
       "14                                                                                                              Cheio de bêbados depois das 2 da manhã :D   \n",
       "15  Recinto sujo, sem manutenção. Não há segurança pra conter a briga que ocorreu agora há pouco. O refrigerante está aguado, sem gosto, horrível. O l...   \n",
       "\n",
       "    rotulo  \n",
       "0     -1.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      1.0  \n",
       "5      0.0  \n",
       "6      0.0  \n",
       "7      1.0  \n",
       "8      1.0  \n",
       "9     -1.0  \n",
       "10     0.0  \n",
       "11     1.0  \n",
       "12    -1.0  \n",
       "13    -1.0  \n",
       "14    -1.0  \n",
       "15    -1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('docs/tips_scenario1_train.csv')\n",
    "df.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'este é um teste de 000 números ! mas que : interessante .'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preProcessing.clean_text('Este é um teste de 354 números! Mas que: \"interessante\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mas', 'que', ':', '\"', 'legal', '\"']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preProcessing.splitWithPunctuation('mas que: \"legal\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1714, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['texto'].astype(str).tolist()\n",
    "categs = df['rotulo'].tolist()\n",
    "texts = [preProcessing.clean_text(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(texts, categs, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVec = CountVectorizer(max_features=4700, lowercase=False, strip_accents='unicode')\n",
    "vectTexts_train = countVec.fit_transform(X_train)\n",
    "vectTexts_test = countVec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'achei': 72,\n",
       " 'comida': 960,\n",
       " 'bem': 498,\n",
       " 'mediocre': 2670,\n",
       " 'prato': 3380,\n",
       " 'com': 929,\n",
       " 'muitas': 2845,\n",
       " 'coisas': 915,\n",
       " 'mas': 2647,\n",
       " 'nada': 2878,\n",
       " 'sabor': 3782,\n",
       " 'nao': 2885,\n",
       " 'vale': 4367,\n",
       " 'que': 3523,\n",
       " 'custa': 1173,\n",
       " 'picburguer': 3272,\n",
       " 'americano': 220,\n",
       " 'sempre': 3883,\n",
       " 'muito': 2847,\n",
       " 'caro': 720,\n",
       " 'pelo': 3191,\n",
       " 'tamanho': 4121,\n",
       " 'fomos': 1889,\n",
       " 'em': 1476,\n",
       " 'dois': 1416,\n",
       " 'gastamos': 2006,\n",
       " '00': 0,\n",
       " 'para': 3105,\n",
       " 'fast': 1781,\n",
       " 'food': 1891,\n",
       " 'de': 1190,\n",
       " 'normal': 2941,\n",
       " 'foi': 1879,\n",
       " 'ojo': 2993,\n",
       " 'del': 1231,\n",
       " 'bifefantastico': 515,\n",
       " 'torta': 4263,\n",
       " 'bacalhau': 406,\n",
       " 'maravilhosa': 2628,\n",
       " 'saladona': 3811,\n",
       " 'opcao': 3017,\n",
       " 'por': 3340,\n",
       " 'reais': 3588,\n",
       " 'no': 2930,\n",
       " 'almoco': 190,\n",
       " 'servico': 3927,\n",
       " 'gentil': 2030,\n",
       " 'unico': 4352,\n",
       " 'problema': 3446,\n",
       " 'demora': 1253,\n",
       " 'desnecessario': 1302,\n",
       " 'colocar': 926,\n",
       " 'todos': 4235,\n",
       " 'os': 3039,\n",
       " 'funcionarios': 1974,\n",
       " 'trabalhar': 4276,\n",
       " 'dia': 1336,\n",
       " 'deveriam': 1329,\n",
       " 'ter': 4176,\n",
       " 'dado': 1176,\n",
       " 'folga': 1881,\n",
       " 'pois': 3325,\n",
       " 'padaria': 3068,\n",
       " 'estava': 1656,\n",
       " 'pouco': 3365,\n",
       " 'movimento': 2830,\n",
       " 'fica': 1828,\n",
       " 'dica': 1341,\n",
       " 'pro': 3445,\n",
       " '0000': 2,\n",
       " 'pessimo': 3251,\n",
       " 'custo': 1174,\n",
       " 'beneficio': 501,\n",
       " 'rodizio': 3729,\n",
       " 'menu': 2701,\n",
       " 'qualidade': 3512,\n",
       " 'do': 1405,\n",
       " 'peixe': 3185,\n",
       " 'baixissima': 427,\n",
       " 'pena': 3193,\n",
       " 'bons': 555,\n",
       " 'cortes': 1102,\n",
       " 'grelhados': 2106,\n",
       " 'especialmente': 1605,\n",
       " 'galeto': 1988,\n",
       " 'buffet': 602,\n",
       " 'saladas': 3809,\n",
       " 'bom': 547,\n",
       " 'destaque': 1316,\n",
       " 'legumes': 2467,\n",
       " 'atendimento': 369,\n",
       " 'rapido': 3575,\n",
       " 'pratos': 3381,\n",
       " 'idem': 2212,\n",
       " 'preco': 3391,\n",
       " 'mais': 2597,\n",
       " 'pela': 3188,\n",
       " 'deliciosa': 1244,\n",
       " 'vezes': 4448,\n",
       " 'desanimo': 1269,\n",
       " 'tentar': 4172,\n",
       " 'comer': 954,\n",
       " 'tomar': 4242,\n",
       " 'cafe': 634,\n",
       " 'enchecao': 1499,\n",
       " 'saco': 3793,\n",
       " 'sentar': 3903,\n",
       " 'nem': 2909,\n",
       " 'organizar': 3034,\n",
       " 'uma': 4344,\n",
       " 'fila': 1842,\n",
       " 'faz': 1791,\n",
       " 'cada': 632,\n",
       " 'um': 4343,\n",
       " 'olho': 3003,\n",
       " 'senta': 3898,\n",
       " 'quem': 3533,\n",
       " 'conseguir': 1042,\n",
       " 'alcancar': 159,\n",
       " 'mesa': 2711,\n",
       " 'primeiro': 3437,\n",
       " 'calor': 667,\n",
       " 'maior': 2593,\n",
       " 'forno': 1902,\n",
       " 'pizza': 3296,\n",
       " 'sair': 3802,\n",
       " 'horas': 2182,\n",
       " 'crepes': 1131,\n",
       " 'justo': 2404,\n",
       " 'lanches': 2452,\n",
       " 'diversas': 1387,\n",
       " 'combinacoes': 933,\n",
       " 'pequeno': 3202,\n",
       " 'tem': 4152,\n",
       " 'mural': 2856,\n",
       " 'pra': 3374,\n",
       " 'deixar': 1224,\n",
       " 'sua': 4047,\n",
       " 'marca': 2633,\n",
       " 'cartao': 728,\n",
       " 'fidelidade': 1835,\n",
       " 'falafel': 1755,\n",
       " 'delicioso': 1247,\n",
       " 'porcao': 3341,\n",
       " 'ou': 3046,\n",
       " 'sanduiche': 3831,\n",
       " 'lanche': 2451,\n",
       " 'vegano': 4400,\n",
       " 'so': 3983,\n",
       " 'pedir': 3177,\n",
       " 'sem': 3880,\n",
       " 'queijos': 3528,\n",
       " 'tambem': 4122,\n",
       " 'otimo': 3044,\n",
       " 'se': 3855,\n",
       " 'chegar': 818,\n",
       " 'almocar': 188,\n",
       " 'cedo': 760,\n",
       " 'consegue': 1039,\n",
       " 'lugar': 2567,\n",
       " 'boa': 530,\n",
       " 'espetinhos': 1622,\n",
       " 'tenham': 4167,\n",
       " 'gosto': 2067,\n",
       " 'tempero': 4161,\n",
       " 'carne': 716,\n",
       " 'baita': 424,\n",
       " 'japa': 2369,\n",
       " 'demorado': 1255,\n",
       " 'atendentes': 362,\n",
       " 'mal': 2598,\n",
       " 'informados': 2292,\n",
       " 'kilo': 2433,\n",
       " 'melhor': 2683,\n",
       " 'ambiente': 210,\n",
       " 'externo': 1735,\n",
       " 'domingo': 1419,\n",
       " 'legal': 2466,\n",
       " 'proposta': 3468,\n",
       " 'salada': 3808,\n",
       " 'na': 2873,\n",
       " 'berrini': 509,\n",
       " 'necessidade': 2899,\n",
       " 'gostei': 2065,\n",
       " 'gostoso': 2073,\n",
       " 'segunda': 3869,\n",
       " 'feira': 1807,\n",
       " 'peixes': 3186,\n",
       " 'frescos': 1942,\n",
       " 'guioza': 2130,\n",
       " 'salmao': 3817,\n",
       " 'salgado': 3815,\n",
       " 'sao': 3835,\n",
       " 'paulo': 3156,\n",
       " 'matar': 2653,\n",
       " 'vontade': 4508,\n",
       " 'sushi': 4099,\n",
       " 'durante': 1443,\n",
       " 'semana': 3881,\n",
       " 'misso': 2760,\n",
       " 'gratis': 2095,\n",
       " 'tao': 4131,\n",
       " 'cobrando': 901,\n",
       " 'banana': 437,\n",
       " 'caramelada': 700,\n",
       " 'das': 1188,\n",
       " 'carnes': 718,\n",
       " 'costelas': 1105,\n",
       " 'suinas': 4071,\n",
       " 'limpeza': 2518,\n",
       " 'familiar': 1767,\n",
       " 'deixa': 1220,\n",
       " 'desejar': 1292,\n",
       " 'excelente': 1695,\n",
       " 'voltar': 4501,\n",
       " 'arroz': 322,\n",
       " 'pato': 3152,\n",
       " 'divino': 1398,\n",
       " 'dos': 1430,\n",
       " 'meus': 2723,\n",
       " 'lugares': 2568,\n",
       " 'preferidos': 3403,\n",
       " 'liberdade': 2500,\n",
       " 'tanto': 4129,\n",
       " 'salgados': 3816,\n",
       " 'qto': 3507,\n",
       " 'doces': 1410,\n",
       " 'fantasticos': 1774,\n",
       " 'hachiberry': 2141,\n",
       " 'delicia': 1239,\n",
       " 'eu': 1679,\n",
       " 'adoro': 122,\n",
       " 'bombom': 550,\n",
       " 'peca': 3160,\n",
       " 'mini': 2750,\n",
       " 'for': 1892,\n",
       " 'sobremesa': 3987,\n",
       " 'tiver': 4220,\n",
       " 'estomago': 1664,\n",
       " 'hehe': 2160,\n",
       " 'taca': 4108,\n",
       " 'colegial': 920,\n",
       " 'perfeita': 3218,\n",
       " 'batata': 470,\n",
       " 'rustica': 3769,\n",
       " 'oferecer': 2986,\n",
       " 'como': 966,\n",
       " 'cortesia': 1103,\n",
       " 'seria': 3919,\n",
       " 'simpatico': 3968,\n",
       " 'pisco': 3293,\n",
       " 'sour': 4024,\n",
       " 'simplemente': 3970,\n",
       " 'sensacional': 3894,\n",
       " 'polvo': 3331,\n",
       " 'crocante': 1140,\n",
       " 'entrada': 1535,\n",
       " 'inesperada': 2284,\n",
       " 'surpresa': 4096,\n",
       " 'imediatamente': 2230,\n",
       " 'levou': 2493,\n",
       " 'minha': 2748,\n",
       " 'avaliacao': 395,\n",
       " 'restaurante': 3694,\n",
       " 'top': 4253,\n",
       " 'da': 1175,\n",
       " 'lista': 2529,\n",
       " '000': 1,\n",
       " 'peruano': 3240,\n",
       " 'costela': 1104,\n",
       " 'bafo': 416,\n",
       " 'mandioca': 2605,\n",
       " 'frita': 1953,\n",
       " 'maravilha': 2626,\n",
       " 'recomendo': 3616,\n",
       " 'kebabes': 2422,\n",
       " 'aqui': 301,\n",
       " 'vc': 4398,\n",
       " 'encontra': 1502,\n",
       " 'variedade': 4391,\n",
       " 'enorme': 1520,\n",
       " 'sabores': 3784,\n",
       " 'philly': 3266,\n",
       " 'steak': 4038,\n",
       " 'menos': 2697,\n",
       " 'queijo': 3527,\n",
       " 'ementhal': 1481,\n",
       " 'amostra': 227,\n",
       " 'pq': 3373,\n",
       " 'sentir': 3908,\n",
       " 'pao': 3099,\n",
       " 'murcho': 2857,\n",
       " 'quantidade': 3515,\n",
       " 'cebola': 757,\n",
       " 'pequena': 3200,\n",
       " 'resumindo': 3699,\n",
       " 'decepcao': 1197,\n",
       " 'total': 4271,\n",
       " 'otima': 3042,\n",
       " 'gosta': 2061,\n",
       " 'japonesa': 2373,\n",
       " 'quer': 3536,\n",
       " 'gastar': 2008,\n",
       " 'vila': 4461,\n",
       " 'olimpia': 3006,\n",
       " 'poderiam': 3320,\n",
       " 'caprichar': 694,\n",
       " 'molhinho': 2788,\n",
       " 'alguns': 178,\n",
       " 'nos': 2946,\n",
       " 'servirmos': 3936,\n",
       " 'hoje': 2173,\n",
       " 'pimentinha': 3280,\n",
       " 'calabresa': 654,\n",
       " 'ficou': 1834,\n",
       " 'faltando': 1762,\n",
       " 'kebab': 2421,\n",
       " 'frango': 1923,\n",
       " 'curry': 1168,\n",
       " 'sugere': 4063,\n",
       " 'informalidade': 2294,\n",
       " 'etc': 1677,\n",
       " 'precos': 3393,\n",
       " 'gourmetizados': 2079,\n",
       " 'limonada': 2515,\n",
       " 'colorida': 928,\n",
       " 'nda': 2896,\n",
       " 'demais': 1251,\n",
       " 'maionese': 2592,\n",
       " 'casa': 735,\n",
       " 'pode': 3316,\n",
       " 'serve': 3923,\n",
       " 'duas': 1439,\n",
       " 'pessoas': 3255,\n",
       " 'hamburguer': 2147,\n",
       " 'tropicalia': 4310,\n",
       " 'fantastico': 1773,\n",
       " 'conferir': 1018,\n",
       " 'ruinzinho': 3765,\n",
       " 'bisteca': 519,\n",
       " 'aceitar': 53,\n",
       " 'credito': 1123,\n",
       " 'american': 217,\n",
       " 'express': 1730,\n",
       " 'centro': 767,\n",
       " 'contrasenso': 1064,\n",
       " 'porcoes': 3343,\n",
       " 'servidas': 3931,\n",
       " 'va': 4363,\n",
       " 'la': 2442,\n",
       " 'sozinho': 4027,\n",
       " 'geral': 2033,\n",
       " 'bureka': 606,\n",
       " 'quanto': 3516,\n",
       " 'falaram': 1759,\n",
       " 'podia': 3321,\n",
       " 'ser': 3916,\n",
       " 'maiorzinho': 2596,\n",
       " 'quase': 3520,\n",
       " 'gnocchi': 2045,\n",
       " 'mandioquinha': 2606,\n",
       " 'ao': 261,\n",
       " 'ragu': 3564,\n",
       " 'mignon': 2737,\n",
       " 'mix': 2768,\n",
       " 'cogumelos': 913,\n",
       " 'harmonizado': 2154,\n",
       " 'montes': 2798,\n",
       " 'reserva': 3680,\n",
       " 'cozinheiro': 1119,\n",
       " 'relaxado': 3662,\n",
       " 'pedi': 3170,\n",
       " 'burrito': 615,\n",
       " 'vegetariano': 4404,\n",
       " 'veio': 4408,\n",
       " 'moida': 2783,\n",
       " 'fui': 1968,\n",
       " 'reclamar': 3605,\n",
       " 'disse': 1381,\n",
       " 'ue': 4334,\n",
       " 'pedido': 3173,\n",
       " 'ta': 4105,\n",
       " 'certo': 774,\n",
       " 'caiu': 647,\n",
       " 'meio': 2680,\n",
       " 'nunca': 2967,\n",
       " 'contemporaneo': 1058,\n",
       " 'show': 3954,\n",
       " 'ceviche': 780,\n",
       " 'pescado': 3246,\n",
       " 'fresco': 1941,\n",
       " 'recomendado': 3610,\n",
       " 'japones': 2372,\n",
       " 'tradicional': 4281,\n",
       " 'dentro': 1263,\n",
       " 'media': 2664,\n",
       " 'mto': 2837,\n",
       " 'feitos': 1812,\n",
       " 'servidos': 3933,\n",
       " 'esta': 1646,\n",
       " 'cheio': 826,\n",
       " 'morango': 2806,\n",
       " 'carnita': 719,\n",
       " 'loca': 2538,\n",
       " 'violento': 4470,\n",
       " 'queima': 3529,\n",
       " 'ate': 352,\n",
       " 'alma': 185,\n",
       " 'espaco': 1594,\n",
       " 'ruim': 3763,\n",
       " 'musica': 2859,\n",
       " 'pessima': 3250,\n",
       " 'compensa': 979,\n",
       " 'toda': 4232,\n",
       " 'qualquer': 3513,\n",
       " 'coisa': 914,\n",
       " 'file': 1844,\n",
       " 'parmegiana': 3124,\n",
       " 'gigante': 2041,\n",
       " 'maravilhoso': 2630,\n",
       " 'ar': 302,\n",
       " 'condicionado': 1012,\n",
       " 'quebrado': 3526,\n",
       " 'sistema': 3978,\n",
       " 'fora': 1893,\n",
       " 'tempo': 4163,\n",
       " 'todo': 4234,\n",
       " 'horrivel': 2185,\n",
       " 'indico': 2278,\n",
       " 'principalmente': 3442,\n",
       " 'varanda': 4381,\n",
       " 'pouca': 3363,\n",
       " 'suco': 4057,\n",
       " 'verde': 4435,\n",
       " 'bastante': 467,\n",
       " 'gengibre': 2025,\n",
       " 'lado': 2445,\n",
       " 'pure': 3496,\n",
       " 'parecia': 3115,\n",
       " 'provo': 3483,\n",
       " 'depois': 1265,\n",
       " 'atencioso': 356,\n",
       " 'correram': 1097,\n",
       " 'quarteirao': 3519,\n",
       " 'me': 2663,\n",
       " 'devolver': 1333,\n",
       " 'casaco': 736,\n",
       " 'esqueci': 1636,\n",
       " 'grande': 2085,\n",
       " 'estacionamento': 1649,\n",
       " 'proprio': 3471,\n",
       " 'outro': 3052,\n",
       " 'rua': 3761,\n",
       " 'chic': 830,\n",
       " 'cobra': 898,\n",
       " 'bauru': 479,\n",
       " 'acompanhamentos': 84,\n",
       " 'coca': 904,\n",
       " 'nhoque': 2921,\n",
       " 'rotisseria': 3751,\n",
       " 'gelados': 2018,\n",
       " 'bolos': 543,\n",
       " 'divinos': 1399,\n",
       " 'poucos': 3366,\n",
       " 'onde': 3008,\n",
       " 'paes': 3073,\n",
       " 'integrais': 2315,\n",
       " 'verdade': 4432,\n",
       " 'gostosos': 2074,\n",
       " 'alem': 166,\n",
       " 'primeira': 3436,\n",
       " 'tudo': 4324,\n",
       " 'itaim': 2355,\n",
       " 'isso': 2353,\n",
       " 'alto': 199,\n",
       " 'claro': 883,\n",
       " 'local': 2539,\n",
       " 'relacao': 3660,\n",
       " 'maluka': 2600,\n",
       " 'cardapio': 707,\n",
       " 'impecavel': 2235,\n",
       " 'trilha': 4299,\n",
       " 'sonora': 4010,\n",
       " 'incrivel': 2270,\n",
       " 'aos': 263,\n",
       " 'finais': 1852,\n",
       " 'provar': 3476,\n",
       " 'mussels': 2865,\n",
       " 'thai': 4199,\n",
       " 'mexilhoes': 2733,\n",
       " 'leite': 2469,\n",
       " 'coco': 907,\n",
       " 'balada': 431,\n",
       " 'garcons': 2001,\n",
       " 'final': 1853,\n",
       " 'noite': 2932,\n",
       " 'pior': 3285,\n",
       " 'ainda': 149,\n",
       " 'confuso': 1025,\n",
       " 'as': 331,\n",
       " 'tortas': 4264,\n",
       " 'boas': 532,\n",
       " 'granola': 2087,\n",
       " 'salgada': 3813,\n",
       " 'vem': 4416,\n",
       " 'previam': 3434,\n",
       " 'resolver': 3687,\n",
       " 'urgentemente': 4357,\n",
       " 'questao': 3544,\n",
       " 'curiosamente': 1166,\n",
       " 'baratos': 449,\n",
       " 'sucos': 4058,\n",
       " 'caros': 724,\n",
       " 'tomates': 4245,\n",
       " 'verdes': 4436,\n",
       " 'fritos': 1957,\n",
       " 'devil': 1331,\n",
       " 'cake': 653,\n",
       " 'dividir': 1395,\n",
       " 'ja': 2361,\n",
       " 'sim': 3964,\n",
       " 'quiser': 3559,\n",
       " 'rico': 3713,\n",
       " 'detalhes': 1320,\n",
       " 'aconchegante': 87,\n",
       " 'jantar': 2368,\n",
       " 'amigos': 224,\n",
       " 'veggie': 4406,\n",
       " 'burguer': 610,\n",
       " 'milkshake': 2742,\n",
       " 'avela': 397,\n",
       " 'vidaaaa': 4453,\n",
       " 'onion': 3012,\n",
       " 'rings': 3721,\n",
       " 'atendida': 365,\n",
       " 'apresentacao': 287,\n",
       " 'risotos': 3723,\n",
       " 'elaborados': 1466,\n",
       " 'simples': 3971,\n",
       " 'agradavel': 136,\n",
       " 'volto': 4505,\n",
       " 'massas': 2649,\n",
       " 'cubana': 1151,\n",
       " 'bife': 514,\n",
       " 'milanesa': 2740,\n",
       " 'servido': 3932,\n",
       " 'achado': 66,\n",
       " 'sp': 4028,\n",
       " 'expresso': 1731,\n",
       " 'fraco': 1913,\n",
       " 'conjunto': 1036,\n",
       " 'pastel': 3149,\n",
       " 'dias': 1340,\n",
       " 'raiva': 3565,\n",
       " 'bela': 493,\n",
       " 'carta': 727,\n",
       " 'vinhos': 4468,\n",
       " 'diversificada': 1389,\n",
       " 'linguica': 2526,\n",
       " 'artesanal': 329,\n",
       " 'javali': 2381,\n",
       " 'entrecote': 1542,\n",
       " 'termine': 4183,\n",
       " 'churros': 865,\n",
       " 'caseiro': 745,\n",
       " 'pedida': 3171,\n",
       " 'bacana': 408,\n",
       " 'tranquilo': 4285,\n",
       " 'mesinhas': 2713,\n",
       " 'java': 2380,\n",
       " 'chip': 843,\n",
       " 'assuste': 350,\n",
       " 'longas': 2551,\n",
       " 'esperas': 1612,\n",
       " 'drinks': 1436,\n",
       " 'entradas': 1536,\n",
       " 'otimas': 3043,\n",
       " 'deixe': 1226,\n",
       " 'sanduba': 3830,\n",
       " 'gyoza': 2132,\n",
       " 'balcao': 434,\n",
       " 'perguntar': 3226,\n",
       " 'qual': 3510,\n",
       " 'ramen': 3567,\n",
       " 'mesmo': 2715,\n",
       " 'tangerina': 4126,\n",
       " 'costuma': 1108,\n",
       " 'aguado': 142,\n",
       " 'opcoes': 3018,\n",
       " 'acai': 49,\n",
       " 'imitacao': 2233,\n",
       " 'don': 1422,\n",
       " 'miguel': 2738,\n",
       " 'esquema': 1638,\n",
       " 'outback': 3049,\n",
       " 'comeco': 948,\n",
       " 'vai': 4366,\n",
       " 'melhorando': 2685,\n",
       " 'estao': 1653,\n",
       " 'idade': 2210,\n",
       " 'pedranestes': 3179,\n",
       " 'tempos': 4164,\n",
       " 'assaltam': 340,\n",
       " 'cego': 761,\n",
       " 'andar': 234,\n",
       " 'dinheiro': 1365,\n",
       " 'fazer': 1795,\n",
       " 'minimo': 2754,\n",
       " 'estupidez': 1675,\n",
       " 'farta': 1776,\n",
       " 'diferente': 1348,\n",
       " 'convencional': 1066,\n",
       " 'paulistano': 3155,\n",
       " 'paladar': 3084,\n",
       " 'ocidental': 2979,\n",
       " 'torcer': 4258,\n",
       " 'nariz': 2886,\n",
       " 'fritas': 1955,\n",
       " 'eh': 1461,\n",
       " 'feijao': 1803,\n",
       " 'wifi': 4521,\n",
       " 'nbsteak00': 2895,\n",
       " 'rigatonni': 3719,\n",
       " 'amatricciana': 209,\n",
       " 'apimentado': 283,\n",
       " 'cafes': 635,\n",
       " 'chocolate': 849,\n",
       " 'fortemente': 1904,\n",
       " 'recomendados': 3611,\n",
       " 'acabei': 46,\n",
       " 'vida': 4452,\n",
       " 'polloloco': 3329,\n",
       " 'comi': 959,\n",
       " 'outros': 3053,\n",
       " 'aki': 153,\n",
       " 'mexicana': 2725,\n",
       " 'agora': 134,\n",
       " 'dar': 1187,\n",
       " 'moderninho': 2778,\n",
       " 'buena': 598,\n",
       " 'suerte': 4060,\n",
       " 'cabron': 625,\n",
       " 'atum': 382,\n",
       " 'esse': 1642,\n",
       " 'acessivel': 64,\n",
       " 'rotatividade': 3750,\n",
       " 'sashimis': 3841,\n",
       " 'nota': 2950,\n",
       " 'servida': 3930,\n",
       " 'temperada': 4157,\n",
       " 'porem': 3344,\n",
       " 'vindo': 4465,\n",
       " 'rong': 3740,\n",
       " 'he': 2159,\n",
       " 'tutoia': 4330,\n",
       " 'estacionar': 1651,\n",
       " 'perto': 3237,\n",
       " 'vegetariana': 4402,\n",
       " 'precisa': 3386,\n",
       " 'graca': 2082,\n",
       " 'soja': 3999,\n",
       " 'mim': 2744,\n",
       " 'goshala': 2060,\n",
       " 'pinheiros': 3283,\n",
       " 'amo': 225,\n",
       " 'vou': 4509,\n",
       " 'sugiro': 4066,\n",
       " 'chai': 784,\n",
       " 'latte': 2461,\n",
       " 'cinnamon': 876,\n",
       " 'roll': 3734,\n",
       " 'evitar': 1686,\n",
       " 'horario': 2180,\n",
       " 'saida': 3798,\n",
       " 'alunos': 202,\n",
       " 'escolas': 1572,\n",
       " 'entorno': 1533,\n",
       " 'loja': 2544,\n",
       " '00h': 5,\n",
       " 'nesse': 2914,\n",
       " 'ficar': 1831,\n",
       " 'interno': 2325,\n",
       " 'bolinho': 539,\n",
       " 'penne': 3195,\n",
       " 'mediterraneo': 2672,\n",
       " 'risotto': 3724,\n",
       " 'aipo': 150,\n",
       " 'excelentes': 1697,\n",
       " 'caprichado': 692,\n",
       " 'combinados': 936,\n",
       " 'teppan': 4175,\n",
       " 'yaki': 4536,\n",
       " 'executivo': 1707,\n",
       " 'teishoko': 4147,\n",
       " 'ume': 4346,\n",
       " 'tempura': 4165,\n",
       " 'anchova': 231,\n",
       " 'grelhada': 2104,\n",
       " 'placa': 3300,\n",
       " 'grata': 2092,\n",
       " 'frequentado': 1936,\n",
       " 'inca': 2256,\n",
       " 'kola': 2438,\n",
       " 'cerveja': 776,\n",
       " 'chocolates': 850,\n",
       " 'peruanos': 3241,\n",
       " 'adivinha': 112,\n",
       " 'picanha': 3270,\n",
       " 'fatiada': 1783,\n",
       " 'polenta': 3327,\n",
       " 'analisar': 229,\n",
       " 'contexto': 1060,\n",
       " 'vir': 4472,\n",
       " 'agil': 132,\n",
       " 'camarao': 671,\n",
       " 'mundo': 2853,\n",
       " 'impossivel': 2242,\n",
       " 'super': 4083,\n",
       " 'mega': 2676,\n",
       " 'recheado': 3598,\n",
       " 'compro': 1000,\n",
       " 'viagem': 4450,\n",
       " 'nas': 2888,\n",
       " 'mesas': 2712,\n",
       " 'minutos': 2757,\n",
       " 'strogonoff': 4045,\n",
       " 'cogumelo': 912,\n",
       " 'paris': 3122,\n",
       " 'pessoa': 3253,\n",
       " 'to': 4223,\n",
       " 'cansada': 680,\n",
       " 'cara': 699,\n",
       " 'porta': 3349,\n",
       " 'ultimas': 4339,\n",
       " 'fechado': 1798,\n",
       " 'ora': 3024,\n",
       " 'reforma': 3640,\n",
       " 'feriado': 1818,\n",
       " 'ferias': 1819,\n",
       " 'coletivas': 922,\n",
       " 'abre': 33,\n",
       " 'horarios': 2181,\n",
       " 'limitados': 2513,\n",
       " 'levam': 2485,\n",
       " 'serio': 3921,\n",
       " 'concorrente': 1007,\n",
       " 'parte': 3127,\n",
       " 'tres': 4298,\n",
       " 'cupuacu': 1164,\n",
       " 'simplesmente': 3972,\n",
       " 'galeria': 1986,\n",
       " 'espaҫo': 1599,\n",
       " 'talher': 4116,\n",
       " 'sobremesas': 3988,\n",
       " 'telao': 4150,\n",
       " 'propaganda': 3466,\n",
       " 'coberto': 896,\n",
       " 'vi': 4449,\n",
       " 'espetos': 1624,\n",
       " 'maquina': 2621,\n",
       " 'sendo': 3886,\n",
       " 'lavados': 2462,\n",
       " 'chao': 795,\n",
       " 'consegui': 1041,\n",
       " 'contar': 1056,\n",
       " 'banheiro': 443,\n",
       " 'imundo': 2250,\n",
       " 'praticamente': 3378,\n",
       " 'paella': 3072,\n",
       " 'acafrao': 48,\n",
       " 'assim': 343,\n",
       " 'seca': 3857,\n",
       " 'lento': 2478,\n",
       " 'tiverem': 4221,\n",
       " 'filial': 1847,\n",
       " 'ceviches': 781,\n",
       " 'descontraidos': 1284,\n",
       " 'experimente': 1723,\n",
       " 'ravioli': 3583,\n",
       " 'brie': 581,\n",
       " 'figo': 1840,\n",
       " 'incriveis': 2269,\n",
       " 'dessa': 1309,\n",
       " 'saraiva': 3839,\n",
       " 'megastore': 2677,\n",
       " 'entao': 1527,\n",
       " 'voce': 4495,\n",
       " 'seu': 3937,\n",
       " 'enquanto': 1523,\n",
       " 'escolhe': 1574,\n",
       " 'livro': 2535,\n",
       " 'comprar': 995,\n",
       " 'le': 2464,\n",
       " 'lo': 2537,\n",
       " 'feliz': 1815,\n",
       " 'starbucks': 4036,\n",
       " 'pop': 3337,\n",
       " 'dizer': 1404,\n",
       " 'cha': 782,\n",
       " 'eles': 1472,\n",
       " 'cobram': 900,\n",
       " 'lotado': 2560,\n",
       " 'senha': 3887,\n",
       " '0000000000': 3,\n",
       " 'parede': 3117,\n",
       " 'otimos': 3045,\n",
       " 'caldos': 660,\n",
       " 'decepcionou': 1203,\n",
       " 'kebabs': 2423,\n",
       " 'estavam': 1657,\n",
       " 'quando': 3514,\n",
       " 'falam': 1756,\n",
       " 'significa': 3962,\n",
       " 'homus': 2175,\n",
       " 'apenas': 277,\n",
       " 'ok': 2994,\n",
       " 'kibe': 2430,\n",
       " 'assado': 338,\n",
       " 'era': 1556,\n",
       " 'charutinho': 802,\n",
       " 'uva': 4362,\n",
       " 'tinha': 4202,\n",
       " 'feno': 1816,\n",
       " 'antes': 254,\n",
       " 'esperando': 1608,\n",
       " 'filas': 1843,\n",
       " 'ficam': 1829,\n",
       " 'enormes': 1521,\n",
       " 'farto': 1778,\n",
       " 'deliciosas': 1246,\n",
       " 'cone': 1016,\n",
       " 'cordeiro': 1084,\n",
       " 'quinta': 3554,\n",
       " 'combo': 939,\n",
       " 'promocional': 3460,\n",
       " 'acompanhamento': 83,\n",
       " 'refrigerante': 3648,\n",
       " 'pecam': 3162,\n",
       " 'catupiry': 753,\n",
       " 'experimentem': 1725,\n",
       " 'deliciosos': 1248,\n",
       " 'coxinha': 1113,\n",
       " 'esquecam': 1632,\n",
       " 'adicionar': 109,\n",
       " 'especial': 1601,\n",
       " 'estiver': 1663,\n",
       " 'fome': 1887,\n",
       " 'acima': 74,\n",
       " 'razoavel': 3587,\n",
       " 'combos': 940,\n",
       " 'valem': 4368,\n",
       " 'deve': 1326,\n",
       " 'sido': 3960,\n",
       " 'algo': 172,\n",
       " 'barulhento': 461,\n",
       " 'momento': 2792,\n",
       " 'flamenco': 1868,\n",
       " 'seguranca': 3872,\n",
       " 'visitei': 4481,\n",
       " 'tenha': 4166,\n",
       " 'mostrado': 2820,\n",
       " 'desinformado': 1297,\n",
       " 'lula': 2570,\n",
       " 'dore': 1428,\n",
       " 'pequenas': 3201,\n",
       " 'altissimo': 198,\n",
       " 'frio': 1949,\n",
       " 'convidativo': 1074,\n",
       " 'doce': 1408,\n",
       " 'ambos': 211,\n",
       " 'grandes': 2086,\n",
       " 'recheados': 3599,\n",
       " 'espera': 1607,\n",
       " 'demorada': 1254,\n",
       " 'mercadao': 2702,\n",
       " 'organizacao': 3030,\n",
       " 'diminuicao': 1359,\n",
       " 'tornou': 4261,\n",
       " 'hocca': 2172,\n",
       " 'muitooo': 2848,\n",
       " 'rs': 3755,\n",
       " 'simpaticos': 3969,\n",
       " 'crepe': 1129,\n",
       " 'nuttela': 2969,\n",
       " 'cartaozinho': 729,\n",
       " 'con': 1002,\n",
       " 'carimbos': 710,\n",
       " 'troco': 4308,\n",
       " 'maravilhaaa': 2627,\n",
       " 'atendido': 367,\n",
       " 'garcom': 1999,\n",
       " 'francke': 1921,\n",
       " 'outra': 3050,\n",
       " 'menina': 2693,\n",
       " 'burger': 608,\n",
       " 'quente': 3534,\n",
       " 'quindim': 3552,\n",
       " 'sigam': 3961,\n",
       " 'twitter': 4332,\n",
       " 'facebook': 1746,\n",
       " 'pizzaria': 3297,\n",
       " 'regiao': 3651,\n",
       " 'ha': 2136,\n",
       " 'maioria': 2595,\n",
       " 'melhores': 2688,\n",
       " 'docinhos': 1413,\n",
       " 'detox': 1323,\n",
       " 'donuts': 1427,\n",
       " 'sagadinhos': 3796,\n",
       " 'ver': 4429,\n",
       " 'frente': 1935,\n",
       " 'prestigiado': 3430,\n",
       " 'alex': 169,\n",
       " 'atala': 351,\n",
       " 'tenho': 4169,\n",
       " 'dificuldade': 1351,\n",
       " 'escolher': 1576,\n",
       " 'parecem': 3114,\n",
       " 'terceira': 4179,\n",
       " 'visita': 4478,\n",
       " 'tristemente': 4303,\n",
       " 'arais': 304,\n",
       " 'diminuido': 1360,\n",
       " 'massa': 2648,\n",
       " 'sirio': 3977,\n",
       " 'industrializada': 2283,\n",
       " 'rap00': 3570,\n",
       " 'mesma': 2714,\n",
       " 'famoso': 1770,\n",
       " 'jeito': 2382,\n",
       " 'focar': 1874,\n",
       " 'limpinho': 2519,\n",
       " 'recinto': 3602,\n",
       " 'sujo': 4077,\n",
       " 'manutencao': 2616,\n",
       " 'conter': 1059,\n",
       " 'briga': 582,\n",
       " 'ocorreu': 2980,\n",
       " 'deixou': 1229,\n",
       " 'essa': 1640,\n",
       " 'churrascaria': 862,\n",
       " 'vive': 4487,\n",
       " 'nome': 2934,\n",
       " 'ultrapassada': 4342,\n",
       " 'outras': 3051,\n",
       " 'fama': 1765,\n",
       " 'impressionar': 2246,\n",
       " 'turistas': 4329,\n",
       " 'bar': 446,\n",
       " 'descolado': 1278,\n",
       " 'mesmos': 2716,\n",
       " 'donos': 1426,\n",
       " 'myk': 2872,\n",
       " 'grega': 2101,\n",
       " 'compensado': 980,\n",
       " 'vinho': 4467,\n",
       " 'grego': 2102,\n",
       " 'clericot': 886,\n",
       " 'evite': 1687,\n",
       " 'carro': 726,\n",
       " 'dificil': 1350,\n",
       " 'achar': 69,\n",
       " 'vaga': 4364,\n",
       " 'hora': 2179,\n",
       " 'cervejas': 777,\n",
       " 'artesanais': 328,\n",
       " 'chopps': 857,\n",
       " 'comidinhas': 963,\n",
       " 'plantas': 3306,\n",
       " 'voltam': 4500,\n",
       " 'criam': 1134,\n",
       " 'guiosa': 2129,\n",
       " 'divina': 1396,\n",
       " 'fecha': 1797,\n",
       " 'poucas': 3364,\n",
       " 'lindo': 2523,\n",
       " 'verdadeiro': 4434,\n",
       " 'jardim': 2376,\n",
       " 'excessos': 1702,\n",
       " 'acredite': 94,\n",
       " 'acessiveis': 63,\n",
       " 'promocoes': 3461,\n",
       " 'interessantes': 2321,\n",
       " 'poderia': 3319,\n",
       " 'melhorar': 2686,\n",
       " 'self': 3879,\n",
       " 'seja': 3874,\n",
       " 'deveria': 1328,\n",
       " 'experimentar': 1722,\n",
       " 'batera': 475,\n",
       " 'quartas': 3518,\n",
       " 'spaguetti': 4030,\n",
       " 'desossado': 1304,\n",
       " 'filet': 1845,\n",
       " 'sente': 3905,\n",
       " 'marcio': 2635,\n",
       " 'chapa': 796,\n",
       " 'co': 892,\n",
       " 'requeijao': 3673,\n",
       " 'preparado': 3417,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countVec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1723],\n",
       "        [  72],\n",
       "        [ 720]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(countVec.transform(['experimente', 'achei', 'caro']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1542x4551 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 25320 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectTexts_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(vectTexts_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111543450064851"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(vectTexts_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7790697674418605"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(vectTexts_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'alpha': [0.001, 0.1, 1, 10, 100], 'fit_prior': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnbParams = { #'verbose' : [1],\n",
    "             'alpha':[0.001, 0.1,1,10, 100],  \n",
    "             'fit_prior' :[True, False]}\n",
    "mnbRSCV = RandomizedSearchCV(mnb, mnbParams, verbose=1, return_train_score=True) #, n_jobs=-1)\n",
    "mnbRSCV.fit(vectTexts_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_fit_prior</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003998</td>\n",
       "      <td>0.003266</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'fit_prior': True, 'alpha': 0.001}</td>\n",
       "      <td>0.753398</td>\n",
       "      <td>0.776699</td>\n",
       "      <td>0.757812</td>\n",
       "      <td>0.762646</td>\n",
       "      <td>0.010113</td>\n",
       "      <td>3</td>\n",
       "      <td>0.988315</td>\n",
       "      <td>0.986368</td>\n",
       "      <td>0.990291</td>\n",
       "      <td>0.988325</td>\n",
       "      <td>0.001602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'fit_prior': False, 'alpha': 0.001}</td>\n",
       "      <td>0.745631</td>\n",
       "      <td>0.761165</td>\n",
       "      <td>0.757812</td>\n",
       "      <td>0.754864</td>\n",
       "      <td>0.006680</td>\n",
       "      <td>4</td>\n",
       "      <td>0.988315</td>\n",
       "      <td>0.984421</td>\n",
       "      <td>0.988350</td>\n",
       "      <td>0.987029</td>\n",
       "      <td>0.001844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003999</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.001884</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'fit_prior': True, 'alpha': 0.1}</td>\n",
       "      <td>0.745631</td>\n",
       "      <td>0.763107</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>0.737354</td>\n",
       "      <td>0.025167</td>\n",
       "      <td>5</td>\n",
       "      <td>0.979552</td>\n",
       "      <td>0.980526</td>\n",
       "      <td>0.986408</td>\n",
       "      <td>0.982162</td>\n",
       "      <td>0.003028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003998</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'fit_prior': False, 'alpha': 0.1}</td>\n",
       "      <td>0.702913</td>\n",
       "      <td>0.714563</td>\n",
       "      <td>0.664062</td>\n",
       "      <td>0.693904</td>\n",
       "      <td>0.021572</td>\n",
       "      <td>7</td>\n",
       "      <td>0.981500</td>\n",
       "      <td>0.976631</td>\n",
       "      <td>0.980583</td>\n",
       "      <td>0.979571</td>\n",
       "      <td>0.002112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003999</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>{'fit_prior': True, 'alpha': 1}</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.791016</td>\n",
       "      <td>0.797017</td>\n",
       "      <td>0.004231</td>\n",
       "      <td>1</td>\n",
       "      <td>0.914314</td>\n",
       "      <td>0.906524</td>\n",
       "      <td>0.909709</td>\n",
       "      <td>0.910182</td>\n",
       "      <td>0.003198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>{'fit_prior': False, 'alpha': 1}</td>\n",
       "      <td>0.782524</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.771484</td>\n",
       "      <td>0.784695</td>\n",
       "      <td>0.011737</td>\n",
       "      <td>2</td>\n",
       "      <td>0.929893</td>\n",
       "      <td>0.927945</td>\n",
       "      <td>0.930097</td>\n",
       "      <td>0.929312</td>\n",
       "      <td>0.000970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.005328</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>{'fit_prior': True, 'alpha': 10}</td>\n",
       "      <td>0.689320</td>\n",
       "      <td>0.687379</td>\n",
       "      <td>0.693359</td>\n",
       "      <td>0.690013</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>8</td>\n",
       "      <td>0.704966</td>\n",
       "      <td>0.697176</td>\n",
       "      <td>0.692233</td>\n",
       "      <td>0.698125</td>\n",
       "      <td>0.005241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>{'fit_prior': False, 'alpha': 10}</td>\n",
       "      <td>0.702913</td>\n",
       "      <td>0.706796</td>\n",
       "      <td>0.714844</td>\n",
       "      <td>0.708171</td>\n",
       "      <td>0.004965</td>\n",
       "      <td>6</td>\n",
       "      <td>0.740019</td>\n",
       "      <td>0.724440</td>\n",
       "      <td>0.722330</td>\n",
       "      <td>0.728930</td>\n",
       "      <td>0.007889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005210</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>{'fit_prior': True, 'alpha': 100}</td>\n",
       "      <td>0.681553</td>\n",
       "      <td>0.681553</td>\n",
       "      <td>0.683594</td>\n",
       "      <td>0.682231</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>10</td>\n",
       "      <td>0.682571</td>\n",
       "      <td>0.682571</td>\n",
       "      <td>0.681553</td>\n",
       "      <td>0.682232</td>\n",
       "      <td>0.000480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>{'fit_prior': False, 'alpha': 100}</td>\n",
       "      <td>0.685437</td>\n",
       "      <td>0.685437</td>\n",
       "      <td>0.689453</td>\n",
       "      <td>0.686770</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>9</td>\n",
       "      <td>0.697176</td>\n",
       "      <td>0.688413</td>\n",
       "      <td>0.689320</td>\n",
       "      <td>0.691636</td>\n",
       "      <td>0.003935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.003998      0.003266         0.001334        0.001887   \n",
       "1       0.002666      0.001885         0.001334        0.001887   \n",
       "2       0.003999      0.000002         0.001332        0.001884   \n",
       "3       0.003998      0.000002         0.000000        0.000000   \n",
       "4       0.003999      0.000002         0.000000        0.000000   \n",
       "5       0.005332      0.001886         0.000000        0.000000   \n",
       "6       0.005328      0.001886         0.000000        0.000000   \n",
       "7       0.001333      0.001886         0.000000        0.000000   \n",
       "8       0.005210      0.007368         0.000000        0.000000   \n",
       "9       0.005208      0.007365         0.000000        0.000000   \n",
       "\n",
       "  param_fit_prior param_alpha                                params  \\\n",
       "0            True       0.001   {'fit_prior': True, 'alpha': 0.001}   \n",
       "1           False       0.001  {'fit_prior': False, 'alpha': 0.001}   \n",
       "2            True         0.1     {'fit_prior': True, 'alpha': 0.1}   \n",
       "3           False         0.1    {'fit_prior': False, 'alpha': 0.1}   \n",
       "4            True           1       {'fit_prior': True, 'alpha': 1}   \n",
       "5           False           1      {'fit_prior': False, 'alpha': 1}   \n",
       "6            True          10      {'fit_prior': True, 'alpha': 10}   \n",
       "7           False          10     {'fit_prior': False, 'alpha': 10}   \n",
       "8            True         100     {'fit_prior': True, 'alpha': 100}   \n",
       "9           False         100    {'fit_prior': False, 'alpha': 100}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "0           0.753398           0.776699           0.757812         0.762646   \n",
       "1           0.745631           0.761165           0.757812         0.754864   \n",
       "2           0.745631           0.763107           0.703125         0.737354   \n",
       "3           0.702913           0.714563           0.664062         0.693904   \n",
       "4           0.800000           0.800000           0.791016         0.797017   \n",
       "5           0.782524           0.800000           0.771484         0.784695   \n",
       "6           0.689320           0.687379           0.693359         0.690013   \n",
       "7           0.702913           0.706796           0.714844         0.708171   \n",
       "8           0.681553           0.681553           0.683594         0.682231   \n",
       "9           0.685437           0.685437           0.689453         0.686770   \n",
       "\n",
       "   std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0        0.010113                3            0.988315            0.986368   \n",
       "1        0.006680                4            0.988315            0.984421   \n",
       "2        0.025167                5            0.979552            0.980526   \n",
       "3        0.021572                7            0.981500            0.976631   \n",
       "4        0.004231                1            0.914314            0.906524   \n",
       "5        0.011737                2            0.929893            0.927945   \n",
       "6        0.002489                8            0.704966            0.697176   \n",
       "7        0.004965                6            0.740019            0.724440   \n",
       "8        0.000961               10            0.682571            0.682571   \n",
       "9        0.001891                9            0.697176            0.688413   \n",
       "\n",
       "   split2_train_score  mean_train_score  std_train_score  \n",
       "0            0.990291          0.988325         0.001602  \n",
       "1            0.988350          0.987029         0.001844  \n",
       "2            0.986408          0.982162         0.003028  \n",
       "3            0.980583          0.979571         0.002112  \n",
       "4            0.909709          0.910182         0.003198  \n",
       "5            0.930097          0.929312         0.000970  \n",
       "6            0.692233          0.698125         0.005241  \n",
       "7            0.722330          0.728930         0.007889  \n",
       "8            0.681553          0.682232         0.000480  \n",
       "9            0.689320          0.691636         0.003935  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(mnbRSCV.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7790697674418605"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnbRSCV.best_estimator_.score(vectTexts_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Embeddings import WordEmbeddingBR, splitWithPunctuation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cbow50_fasttext', 'cbow50_wang2vec', 'glove50']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordEmbeddingBR.downloadNILCEmbeddings()\n",
    "WordEmbeddingBR.getAvailableEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embedding file: cbow50_wang2vec.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "934967it [01:08, 13550.89it/s]\n"
     ]
    }
   ],
   "source": [
    "wee = WordEmbeddingBR('cbow50_wang2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Support Vector Machine...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:    7.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]Fitting Gradient Boosted Tree...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         658.5195            6.26s\n",
      "         2         495.4146            6.12s\n",
      "         3         380.5770            6.32s\n",
      "         4         297.1064            6.83s\n",
      "         5         243.7370            6.74s\n",
      "         6         212.2517            6.74s\n",
      "         7         179.5781            6.47s\n",
      "         8         152.4104            6.27s\n",
      "         9         132.1538            6.12s\n",
      "        10         115.4280            5.95s\n",
      "        20          37.9122            4.85s\n",
      "        30          13.3994            4.19s\n",
      "        40           5.1836            3.59s\n",
      "        50           1.8979            3.04s\n",
      "        60           0.8089            2.58s\n",
      "        70           0.3634            2.07s\n",
      "        80           0.2790            1.48s\n",
      "        90           0.2790            0.95s\n",
      "       100           0.2790            0.52s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         671.9916            4.55s\n",
      "         2         520.6709            5.02s\n",
      "         3         406.8037            5.31s\n",
      "         4         326.3749            5.82s\n",
      "         5         266.0991            6.30s\n",
      "         6         225.0647            6.47s\n",
      "         7         195.0785            6.46s\n",
      "         8         164.2433            6.45s\n",
      "         9         143.5765            6.25s\n",
      "        10         125.4225            6.08s\n",
      "        20          43.1297            4.77s\n",
      "        30          18.9884            3.98s\n",
      "        40           9.5922            3.37s\n",
      "        50           5.9567            2.91s\n",
      "        60           4.1062            2.54s\n",
      "        70           3.4722            2.07s\n",
      "        80           3.1418            1.61s\n",
      "        90           3.0220            1.10s\n",
      "       100           2.9930            0.62s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         660.5007            6.31s\n",
      "         2         507.4379            6.51s\n",
      "         3         399.0890            6.06s\n",
      "         4         315.9342            5.75s\n",
      "         5         253.6216            5.93s\n",
      "         6         212.1103            6.04s\n",
      "         7         178.4741            6.15s\n",
      "         8         156.5863            6.10s\n",
      "         9         135.0496            6.02s\n",
      "        10         120.4958            5.85s\n",
      "        20          41.9691            4.60s\n",
      "        30          16.6314            3.85s\n",
      "        40           7.9419            3.36s\n",
      "        50           5.0688            2.89s\n",
      "        60           3.7268            2.41s\n",
      "        70           3.2481            1.93s\n",
      "        80           3.0518            1.45s\n",
      "        90           3.0093            0.98s\n",
      "       100           2.9808            0.55s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         770.1586            7.27s\n",
      "         2         670.9554            9.79s\n",
      "         3         603.4396            9.38s\n",
      "         4         547.4962            9.00s\n",
      "         5         506.8908            9.22s\n",
      "         6         471.2818            9.03s\n",
      "         7         437.2783            8.91s\n",
      "         8         407.9026            8.83s\n",
      "         9         387.2833            8.77s\n",
      "        10         367.8681            8.61s\n",
      "        20         224.1091            7.48s\n",
      "        30         143.1032            7.14s\n",
      "        40          99.1938            6.88s\n",
      "        50          70.6351            6.51s\n",
      "        60          51.6205            6.21s\n",
      "        70          37.3589            5.96s\n",
      "        80          28.9748            5.65s\n",
      "        90          21.2176            5.35s\n",
      "       100          15.7538            5.06s\n",
      "       200           0.9375            2.44s\n",
      "       300           0.2616            0.20s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         781.8112            8.97s\n",
      "         2         683.8254            8.33s\n",
      "         3         609.8548            9.22s\n",
      "         4         562.0009            9.46s\n",
      "         5         521.6649            9.87s\n",
      "         6         488.4876           10.17s\n",
      "         7         452.1422           10.25s\n",
      "         8         427.2836           10.19s\n",
      "         9         403.6289           10.00s\n",
      "        10         382.1124           10.02s\n",
      "        20         241.6303            9.43s\n",
      "        30         161.6753            8.69s\n",
      "        40         116.7451            8.00s\n",
      "        50          82.0145            7.29s\n",
      "        60          61.7570            6.87s\n",
      "        70          47.2591            6.55s\n",
      "        80          35.1193            6.20s\n",
      "        90          27.5742            5.91s\n",
      "       100          21.1813            5.60s\n",
      "       200           4.3264            2.51s\n",
      "       300           3.0367            0.23s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         774.9551            7.12s\n",
      "         2         670.2225            7.63s\n",
      "         3         604.7745            8.15s\n",
      "         4         556.1663            8.27s\n",
      "         5         508.8844            8.74s\n",
      "         6         470.0089            8.73s\n",
      "         7         444.2526            8.91s\n",
      "         8         415.0768            8.71s\n",
      "         9         385.9428            8.61s\n",
      "        10         366.1157            8.47s\n",
      "        20         230.4047            7.15s\n",
      "        30         148.4890            6.56s\n",
      "        40         103.4973            6.16s\n",
      "        50          75.0101            5.82s\n",
      "        60          56.4689            5.51s\n",
      "        70          43.1838            5.23s\n",
      "        80          32.6514            4.99s\n",
      "        90          25.3975            4.74s\n",
      "       100          20.0868            4.61s\n",
      "       200           4.2903            2.26s\n",
      "       300           3.0045            0.21s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         892.2209            9.81s\n",
      "         2         832.6355           10.22s\n",
      "         3         780.8232            9.91s\n",
      "         4         735.7237            9.96s\n",
      "         5         694.6419            9.95s\n",
      "         6         658.9340           10.13s\n",
      "         7         626.5421            9.98s\n",
      "         8         596.1736            9.86s\n",
      "         9         570.4062            9.96s\n",
      "        10         546.9959           10.32s\n",
      "        20         362.2477            9.04s\n",
      "        30         260.8026            8.16s\n",
      "        40         195.7642            7.51s\n",
      "        50         154.8390            6.96s\n",
      "        60         121.5850            6.50s\n",
      "        70          97.5654            6.08s\n",
      "        80          79.3683            5.63s\n",
      "        90          65.1696            5.21s\n",
      "       100          54.3592            4.81s\n",
      "       200           8.7824            0.98s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         896.7241           10.60s\n",
      "         2         842.6213           10.16s\n",
      "         3         792.6049           10.55s\n",
      "         4         748.9750           11.30s\n",
      "         5         708.4118           11.23s\n",
      "         6         670.8958           11.18s\n",
      "         7         637.7008           11.04s\n",
      "         8         606.0905           10.82s\n",
      "         9         578.6467           10.98s\n",
      "        10         554.3936           10.74s\n",
      "        20         375.1972            9.68s\n",
      "        30         268.7083            9.15s\n",
      "        40         201.4377            9.53s\n",
      "        50         155.3608            8.97s\n",
      "        60         123.4038            8.20s\n",
      "        70          99.1230            7.92s\n",
      "        80          79.7592            7.36s\n",
      "        90          66.0932            6.76s\n",
      "       100          54.3352            6.13s\n",
      "       200          11.4773            1.16s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         898.1110            9.92s\n",
      "         2         838.4991           10.16s\n",
      "         3         787.0119            9.95s\n",
      "         4         742.1632           11.07s\n",
      "         5         701.6666           11.34s\n",
      "         6         664.3818           11.10s\n",
      "         7         627.0572           11.10s\n",
      "         8         597.5516           11.01s\n",
      "         9         566.8869           11.24s\n",
      "        10         540.0326           11.02s\n",
      "        20         347.1043           10.30s\n",
      "        30         244.9993            9.07s\n",
      "        40         182.6824            8.28s\n",
      "        50         140.6383            7.60s\n",
      "        60         109.2661            7.08s\n",
      "        70          88.5923            6.55s\n",
      "        80          71.3518            6.05s\n",
      "        90          57.6216            5.58s\n",
      "       100          47.1368            5.15s\n",
      "       200           8.3875            1.06s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         663.8706           37.10s\n",
      "         2         475.0937           38.23s\n",
      "         3         346.4510           39.12s\n",
      "         4         251.0832           38.49s\n",
      "         5         185.3167           38.60s\n",
      "         6         141.1186           38.26s\n",
      "         7         106.7624           38.85s\n",
      "         8          79.6621           38.81s\n",
      "         9          58.7069           38.76s\n",
      "        10          43.7401           38.67s\n",
      "        20           3.4129           34.41s\n",
      "        30           0.4175           30.69s\n",
      "        40           0.2975           23.91s\n",
      "        50           0.2975           18.71s\n",
      "        60           0.2975           15.27s\n",
      "        70           0.2975           12.76s\n",
      "        80           0.2975           10.87s\n",
      "        90           0.2975            9.39s\n",
      "       100           0.2975            8.20s\n",
      "       200           0.2975            2.70s\n",
      "       300           0.2975            0.69s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         652.7229           37.81s\n",
      "         2         462.3991           37.70s\n",
      "         3         327.6977           38.70s\n",
      "         4         251.4068           38.76s\n",
      "         5         190.7176           38.53s\n",
      "         6         140.2635           40.29s\n",
      "         7         108.6628           40.17s\n",
      "         8          83.4745           40.67s\n",
      "         9          62.1611           41.36s\n",
      "        10          48.5491           41.29s\n",
      "        20           6.2769           38.09s\n",
      "        30           3.2326           34.02s\n",
      "        40           3.0318           27.61s\n",
      "        50           2.9634           23.12s\n",
      "        60           2.9333           19.92s\n",
      "        70           2.8976           17.50s\n",
      "        80           2.8726           15.80s\n",
      "        90           2.8528           14.43s\n",
      "       100           2.8386           13.11s\n",
      "       200           2.7803            5.90s\n",
      "       300           2.7736            1.84s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         635.9633           42.93s\n",
      "         2         450.8125           44.13s\n",
      "         3         321.9947           46.49s\n",
      "         4         232.8121           47.78s\n",
      "         5         169.8600           46.25s\n",
      "         6         122.2124           45.52s\n",
      "         7          90.3944           44.81s\n",
      "         8          66.9604           44.96s\n",
      "         9          49.1772           44.55s\n",
      "        10          36.8992           43.90s\n",
      "        20           5.6840           36.36s\n",
      "        30           3.2064           31.28s\n",
      "        40           3.0242           25.27s\n",
      "        50           2.9581           21.24s\n",
      "        60           2.9237           18.24s\n",
      "        70           2.8905           16.11s\n",
      "        80           2.8649           14.49s\n",
      "        90           2.8458           13.20s\n",
      "       100           2.8311           12.11s\n",
      "       200           2.7797            5.59s\n",
      "       300           2.7734            1.83s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1046.0737            7.97s\n",
      "         2         848.6318            7.89s\n",
      "         3         705.4500            7.73s\n",
      "         4         598.5587            7.87s\n",
      "         5         497.5283            7.63s\n",
      "         6         436.0509            7.43s\n",
      "         7         381.8273            7.55s\n",
      "         8         335.0726            7.40s\n",
      "         9         299.2200            7.25s\n",
      "        10         270.3118            7.29s\n",
      "        20         105.2294            6.11s\n",
      "        30          52.3799            5.49s\n",
      "        40          26.7224            4.86s\n",
      "        50          15.5067            4.24s\n",
      "        60          10.4264            3.51s\n",
      "        70           7.7755            2.80s\n",
      "        80           6.2265            2.13s\n",
      "        90           5.3279            1.50s\n",
      "       100           4.8589            0.89s\n"
     ]
    }
   ],
   "source": [
    "classifiers = wee.TrainBaselineClassifiers(X_train, y_train, n_iter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SVM': 0.9980544747081712, 'GradientBoostingClassifier': 0.9980544747081712}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wee.TestBaselineClassifiers(X_train, y_train, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SVM': 0.7732558139534884, 'GradientBoostingClassifier': 0.7558139534883721}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wee.TestBaselineClassifiers(X_test, y_test, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8081395348837209"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmRSCV.best_estimator_.score(vectTexts_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "Base: Bidirectional Attention Flow for Machine Comprehension https://arxiv.org/abs/1611.01603\n",
    "\n",
    "TODO: Write data generator, model compatible with scikit-learn RandomSearchCV\n",
    "\n",
    "Inputs to the model:\n",
    "\n",
    "- Integer codes of each word\n",
    "- Integer codes of each character of each word\n",
    "- Pretrained embeddings for each word\n",
    "\n",
    "## Step 1: Encode characters\n",
    "\n",
    "For the character embedding layer, we will use all available characters plus the [PAD] character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dougl_000\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "##############\n",
    "#Dictionaries\n",
    "##############\n",
    "\n",
    "#for all dicts, add 1 to result and reserve 0 to not found\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "\n",
    "allchars = string.printable\n",
    "allchars = [x for x in allchars] + ['PAD']\n",
    "allchars = { allchars[i]:i for i in range(len(allchars)) if allchars[i] not in [' ','\\n']}\n",
    "\n",
    "def extractVocabulary(textSet, maxWords = 3000):\n",
    "    #extracts vocabulary from a list of texts\n",
    "    #preprocessing to remove accents and uppercase should be done before\n",
    "    \n",
    "    countVec = CountVectorizer(max_features=maxWords, lowercase=False, strip_accents=None)\n",
    "    countVec.fit(textSet)\n",
    "    \n",
    "    #append punctuation\n",
    "    vocab = countVec.vocabulary_\n",
    "    n = len(vocab)\n",
    "    for x in string.punctuation:\n",
    "        vocab[x]=n\n",
    "        n += 1\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def sentence2code(sentence, vocabulary, embClass = None):\n",
    "    \"\"\"\n",
    "    Converts a sentence to char embedding codes, word embedding codes and embeddings\n",
    "    \n",
    "    sentence - list of words in the sentence, usually from preProcessing.clean_text().split(' ')\n",
    "    embClass - a class that implements method encodeWord and has property embDim (embedding dimension)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert type(sentence) == list, 'sentence should be a list of words'\n",
    "    \n",
    "    #sentence\n",
    "    sentCode = [vocabulary.get(w,-1)+1 for w in sentence]\n",
    "    \n",
    "    #characters\n",
    "    sent_len = len(sentence)\n",
    "    maxwlen = np.max([len(x) for x in sentence])\n",
    "    charCodes = np.zeros( (sent_len, maxwlen) ) + allchars['PAD']\n",
    "    \n",
    "    for i in range(sent_len):\n",
    "        charEnc = [allchars.get(cc, -1)+1 for cc in sentence[i]]\n",
    "        charCodes[i, 0:len(charEnc)] = charEnc\n",
    "    \n",
    "    wordEmbeddings = None\n",
    "    if embClass is not None:\n",
    "        wordEmbeddings = np.zeros ((sent_len, embClass.embDim))\n",
    "        for i in range(sent_len):\n",
    "            wordEmbeddings[i] = embClass.encodeWord(sentence[i])\n",
    "        \n",
    "    return np.array(sentCode), charCodes.astype(int), wordEmbeddings\n",
    "\n",
    "\n",
    "##############\n",
    "#Keras models\n",
    "##############\n",
    "#change LSTM to CuDNNLSTM\n",
    "from keras.layers import CuDNNLSTM as LSTM\n",
    "from keras.layers import Input, Embedding, Conv2D, Lambda, Concatenate, Bidirectional, TimeDistributed\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "\n",
    "def createCharEncoder(charDictSize, embSize, nFiltersNGram=16, filterSize = 5):\n",
    "    \"\"\"\n",
    "    Creates a character encoder. Receives the integer code of the character.\n",
    "    \n",
    "    charDictSize - Length of dictionary of characters\n",
    "    embSize - Embedding size\n",
    "    \"\"\"\n",
    "    inp = Input((None, ))\n",
    "    \n",
    "    embedded = Embedding(charDictSize, embSize)(inp)\n",
    "        \n",
    "    embedded = Lambda(lambda x: K.expand_dims(x))(embedded)\n",
    "    ngram = Conv2D(nFiltersNGram, kernel_size = (5,1), padding='same', activation='relu')(embedded)\n",
    "    ngram = Conv2D(1, kernel_size = (filterSize,1), padding='same', activation=None)(ngram)\n",
    "    \n",
    "    ngram = Lambda(lambda x: K.squeeze(x, axis=3))(ngram)\n",
    "    ngram = Bidirectional(LSTM(embSize))(ngram)\n",
    "    \n",
    "    output = ngram\n",
    "    \n",
    "    model = Model(inputs=[inp], outputs=[output], name='CharEncoder')\n",
    "    return model\n",
    "\n",
    "def createDocEncoder(dictSize, embSize, nFiltersWordGram = 10, filterSize = 5, embDim = None):\n",
    "    \"\"\"\n",
    "    Creates a document encoder. Receives the integer code of the words.\n",
    "    \n",
    "    dictSize - Length of word dictionary\n",
    "    embSize - Embedding size\n",
    "    \"\"\"\n",
    "    inp = Input((None, ))\n",
    "    \n",
    "    embedded = Embedding(dictSize, embSize)(inp)\n",
    "    \n",
    "    #combine learned and pretrained embeddings\n",
    "    if embDim is not None:\n",
    "        preTrainedEmb = Input((None, embDim))\n",
    "        embedded = Concatenate()([embedded, preTrainedEmb])\n",
    "\n",
    "    \n",
    "    embedded = Lambda(lambda x: K.expand_dims(x))(embedded)\n",
    "    ngram = Conv2D(nFiltersWordGram, kernel_size = (filterSize,1), padding='same', activation='relu')(embedded)\n",
    "    ngram = Conv2D(1, kernel_size = (filterSize,1), padding='same', activation=None)(ngram)\n",
    "    \n",
    "    ngram = Lambda(lambda x: K.squeeze(x, axis=3))(ngram)\n",
    "    ngram = Bidirectional(LSTM(embSize//2, return_sequences=True))(ngram)\n",
    "    \n",
    "    output = ngram\n",
    "    \n",
    "    if embDim is None:\n",
    "        model = Model(inputs=[inp], outputs=[output], name='WordEncoder')\n",
    "    else:\n",
    "        model = Model(inputs=[inp, preTrainedEmb], outputs=[output], name='WordEncoderWithPreEmb')\n",
    "    return model  \n",
    "\n",
    "\n",
    "##############################\n",
    "#Scikit learn compatible model\n",
    "##############################\n",
    "\n",
    "\n",
    "\n",
    "class BiDirAttModel(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, charEmbeddingDim = 'auto'):\n",
    "        \"\"\"\n",
    "        Initializes classifier\n",
    "        \n",
    "        charEmbeddingDim - desired character embedding dimension or sqrt(len(allchars)) if 'auto'\n",
    "        \"\"\"\n",
    "        self.charEmbeddingDim = charEmbeddingDim\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if self.charEmbeddingDim == 'auto':\n",
    "            self.charEmbeddingDim_ = int(np.sqrt(len(allchars)))\n",
    "        else:\n",
    "            self.charEmbeddingDim_ = self.charEmbeddingDim\n",
    "        \n",
    "\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self, ['X_', 'y_'])\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "\n",
    "        closest = np.argmin(euclidean_distances(X, self.X_), axis=1)\n",
    "        return self.y_[closest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bons cortes grelhados , especialmente o galeto . buffet de saladas bom , com destaque para os legumes grelhados . atendimento rápido e pratos idem . preço mais caro , mas vale pela comida deliciosa .'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = extractVocabulary(X_train, maxWords=10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[8].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.476257, -0.258659, -0.091403, ..., -0.017869, -0.636823,\n",
       "         0.397816],\n",
       "       [ 0.53667 , -0.677689, -0.02436 , ...,  0.62127 ,  0.219901,\n",
       "         0.541482],\n",
       "       [-0.71601 , -0.611201,  0.515311, ...,  0.061538,  0.031443,\n",
       "         0.326067],\n",
       "       ...,\n",
       "       [-0.643728, -0.183329,  0.17764 , ..., -0.214589,  0.809557,\n",
       "        -0.426949],\n",
       "       [-0.07863 , -0.374874, -0.634451, ..., -0.425417,  0.110938,\n",
       "        -0.018094],\n",
       "       [-0.185625, -0.287698, -0.240053, ..., -0.028405,  0.298558,\n",
       "        -0.006506]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2code(X_train[8].split(' '), vocab, embClass=wee)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 200)    800000      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, None, 50)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, None, 250)    0           embedding_5[0][0]                \n",
      "                                                                 input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, None, 250, 1) 0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, 250, 10 60          lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, 250, 1) 51          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, None, 250)    0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, None, 200)    281600      lambda_10[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,081,711\n",
      "Trainable params: 1,081,711\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m=createDocEncoder(4000,200, embDim=50)\n",
    "m.summary()\n",
    "plot_model(m, to_file='wordEncoder.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, None, 16)          1600      \n",
      "_________________________________________________________________\n",
      "lambda_11 (Lambda)           (None, None, 16, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, None, 16, 16)      96        \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, None, 16, 1)       81        \n",
      "_________________________________________________________________\n",
      "lambda_12 (Lambda)           (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 32)                4352      \n",
      "=================================================================\n",
      "Total params: 6,129\n",
      "Trainable params: 6,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = createCharEncoder(100, 16)\n",
    "m.summary()\n",
    "plot_model(m, to_file='characterEncoder.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBiDirAttModel(charDictSize, dictSize,\n",
    "                        charEmbSize=16, nFiltersNGram=16, charfilterSize = 5, #character params\n",
    "                        wordEmbSize=128, nFiltersWordGram = 10, wordfilterSize = 5, preTrainedEmbDim = None): #word params\n",
    "    \n",
    "    inputChars = Input((None, None))\n",
    "    cFeatLayer = createCharEncoder(charDictSize, charEmbSize, nFiltersNGram, charfilterSize)\n",
    "    charFeats = TimeDistributed(cFeatLayer)(inputChars)\n",
    "    \n",
    "    inputWords = Input((None, ))\n",
    "    if preTrainedEmbDim is not None:\n",
    "        preTrainedEmb = Input((None, preTrainedEmbDim))\n",
    "        wordEncoded = createDocEncoder(dictSize, wordEmbSize, nFiltersWordGram, \n",
    "                                       wordfilterSize, preTrainedEmbDim)([inputWords, preTrainedEmb])\n",
    "    else:\n",
    "        wordEncoded = createDocEncoder(dictSize, wordEmbSize, nFiltersWordGram, \n",
    "                                       wordfilterSize, preTrainedEmbDim)(inp)\n",
    "        \n",
    "    output = Concatenate()([wordEncoded, charFeats])\n",
    "    if preTrainedEmbDim is None:\n",
    "        model = Model(inputs=[inputWords], outputs=[output], name='BiAttEnc')\n",
    "    else:\n",
    "        model = Model(inputs=[inputWords, inputChars, preTrainedEmb], outputs=[output], name='BiAttEncWithPretrainedEmb')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, None, 50)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "WordEncoderWithPreEmb (Model)   (None, None, 128)    509039      input_6[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 32)     4785        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 160)    0           WordEncoderWithPreEmb[1][0]      \n",
      "                                                                 time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 513,824\n",
      "Trainable params: 513,824\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = createBiDirAttModel(16, 3000, preTrainedEmbDim=50)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\dougl_000\\\\Anaconda3;C:\\\\Users\\\\dougl_000\\\\Anaconda3\\\\Library\\\\mingw-w64\\\\bin;C:\\\\Users\\\\dougl_000\\\\Anaconda3\\\\Library\\\\usr\\\\bin;C:\\\\Users\\\\dougl_000\\\\Anaconda3\\\\Library\\\\bin;C:\\\\Users\\\\dougl_000\\\\Anaconda3\\\\Scripts;C:\\\\Users\\\\dougl_000\\\\Anaconda3\\\\Library\\\\bin;C:\\\\ProgramData\\\\Oracle\\\\Java\\\\javapath;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\Program Files (x86)\\\\GtkSharp\\\\2.12\\\\bin;C:\\\\Program Files\\\\SafeNet\\\\Authentication\\\\SAC\\\\x32;C:\\\\Program Files\\\\SafeNet\\\\Authentication\\\\SAC\\\\x64;C:\\\\Program Files (x86)\\\\Windows Live\\\\Shared;C:\\\\Program Files (x86)\\\\Skype\\\\Phone\\\\;C:\\\\Program Files\\\\dotnet\\\\;C:\\\\Program Files\\\\Microsoft SQL Server\\\\130\\\\Tools\\\\Binn\\\\;C:\\\\WINDOWS\\\\System32\\\\OpenSSH\\\\;C:\\\\Program Files\\\\doxygen\\\\bin;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Users\\\\dougl_000\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;;C:\\\\Users\\\\dougl_000\\\\Anaconda3\\\\lib\\\\site-packages\\\\numpy\\\\.libs;C:/Program Files (x86)/Graphviz2.38/bin/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(m, to_file='BidirAtt.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\dougl_000\\appdata\\roaming\\python\\python36\\site-packages (0.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 16, 16)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.predict(np.array([[0,2,5,6,1],[0,2,5,1,99]])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.svm import LinearSVC\n",
    "check_estimator(BiDirAttModel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
