{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Download the Foursquare annotated comments in Brazilian Portuguese: https://www.kaggle.com/thaisalmeida/tips-foursquare/version/1\n",
    "\n",
    "Place the files in subfolder 'docs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget files if using Google Colab\n",
    "!wget -q https://raw.githubusercontent.com/douglas125/TextClassification/master/preProcessing.py\n",
    "!wget -q https://raw.githubusercontent.com/douglas125/TextClassification/master/Embeddings.py\n",
    "!wget -q https://raw.githubusercontent.com/douglas125/TextClassification/master/requirements.txt\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "\n",
    "#move CSVs to docs/ folder\n",
    "from google.colab import files\n",
    "files.upload()\n",
    "\n",
    "!mkdir docs\n",
    "!mv *.csv docs/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import preProcessing\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "pd.set_option('max_colwidth',150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>rotulo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A comida é deliciosa, mas pedi limonada suiça e me disseram que hoje estavam todos muito ocupados e que ninguém conseguiria me atender....melhor i...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A partir desta sexta feira dia 11 começam a abrir para jantar mas corre pois é só até as 22 hrs e no domingo dia das mães, estarão aberto durante ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joint burguer e brewdog</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agora de segunda a sexta o Habanero vai abrir no almoço com pratos mexicanos e tradicionais!</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Experimente o drink \"Dona Diabla\". Muito bom!</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nova senha do Wifi: 1129508219</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wi-fi 1129508219</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Adoramos a pizza carbonara e a paulistana. Não surpreendeu tanto, mas vale a pena por resgatar o tradicionalismo. Dica @Gourmet_For</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O diferencial desse Burger King é que você mesmo serve o refrigerante, e a vontade!</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Unico defeito estacionamento pago!</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24h é 24h.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Excelente Burger King: bom atendimento, ambiente agradável e refil para refrigerante. Evite o inconveniente estacionamento pago estacionando ao re...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>o atendimento aqui é simplesmente um lixo</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>O Drive-Truh mais lento da cidade!!</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cheio de bêbados depois das 2 da manhã :D</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Recinto sujo, sem manutenção. Não há segurança pra conter a briga que ocorreu agora há pouco. O refrigerante está aguado, sem gosto, horrível. O l...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    texto  \\\n",
       "0   A comida é deliciosa, mas pedi limonada suiça e me disseram que hoje estavam todos muito ocupados e que ninguém conseguiria me atender....melhor i...   \n",
       "1   A partir desta sexta feira dia 11 começam a abrir para jantar mas corre pois é só até as 22 hrs e no domingo dia das mães, estarão aberto durante ...   \n",
       "2                                                                                                                                 Joint burguer e brewdog   \n",
       "3                                                            Agora de segunda a sexta o Habanero vai abrir no almoço com pratos mexicanos e tradicionais!   \n",
       "4                                                                                                           Experimente o drink \"Dona Diabla\". Muito bom!   \n",
       "5                                                                                                                          Nova senha do Wifi: 1129508219   \n",
       "6                                                                                                                                        Wi-fi 1129508219   \n",
       "7                     Adoramos a pizza carbonara e a paulistana. Não surpreendeu tanto, mas vale a pena por resgatar o tradicionalismo. Dica @Gourmet_For   \n",
       "8                                                                     O diferencial desse Burger King é que você mesmo serve o refrigerante, e a vontade!   \n",
       "9                                                                                                                      Unico defeito estacionamento pago!   \n",
       "10                                                                                                                                             24h é 24h.   \n",
       "11  Excelente Burger King: bom atendimento, ambiente agradável e refil para refrigerante. Evite o inconveniente estacionamento pago estacionando ao re...   \n",
       "12                                                                                                              o atendimento aqui é simplesmente um lixo   \n",
       "13                                                                                                                    O Drive-Truh mais lento da cidade!!   \n",
       "14                                                                                                              Cheio de bêbados depois das 2 da manhã :D   \n",
       "15  Recinto sujo, sem manutenção. Não há segurança pra conter a briga que ocorreu agora há pouco. O refrigerante está aguado, sem gosto, horrível. O l...   \n",
       "\n",
       "    rotulo  \n",
       "0     -1.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      1.0  \n",
       "5      0.0  \n",
       "6      0.0  \n",
       "7      1.0  \n",
       "8      1.0  \n",
       "9     -1.0  \n",
       "10     0.0  \n",
       "11     1.0  \n",
       "12    -1.0  \n",
       "13    -1.0  \n",
       "14    -1.0  \n",
       "15    -1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('docs/tips_scenario1_train.csv')\n",
    "df.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'este é um teste de 000 números ! mas que : interessante .'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preProcessing.clean_text('Este é um teste de 354 números! Mas que: \"interessante\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mas', 'que', ':', '\"', 'legal', '\"']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preProcessing.splitWithPunctuation('mas que: \"legal\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1714, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['texto'].astype(str).tolist()\n",
    "categs = df['rotulo'].tolist()\n",
    "texts = [preProcessing.clean_text(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(texts, categs, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVec = CountVectorizer(max_features=4700, lowercase=False, strip_accents='unicode')\n",
    "vectTexts_train = countVec.fit_transform(X_train)\n",
    "vectTexts_test = countVec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'achei': 72,\n",
       " 'comida': 960,\n",
       " 'bem': 498,\n",
       " 'mediocre': 2670,\n",
       " 'prato': 3380,\n",
       " 'com': 929,\n",
       " 'muitas': 2845,\n",
       " 'coisas': 915,\n",
       " 'mas': 2647,\n",
       " 'nada': 2878,\n",
       " 'sabor': 3782,\n",
       " 'nao': 2885,\n",
       " 'vale': 4367,\n",
       " 'que': 3523,\n",
       " 'custa': 1173,\n",
       " 'picburguer': 3272,\n",
       " 'americano': 220,\n",
       " 'sempre': 3883,\n",
       " 'muito': 2847,\n",
       " 'caro': 720,\n",
       " 'pelo': 3191,\n",
       " 'tamanho': 4121,\n",
       " 'fomos': 1889,\n",
       " 'em': 1476,\n",
       " 'dois': 1416,\n",
       " 'gastamos': 2006,\n",
       " '00': 0,\n",
       " 'para': 3105,\n",
       " 'fast': 1781,\n",
       " 'food': 1891,\n",
       " 'de': 1190,\n",
       " 'normal': 2941,\n",
       " 'foi': 1879,\n",
       " 'ojo': 2993,\n",
       " 'del': 1231,\n",
       " 'bifefantastico': 515,\n",
       " 'torta': 4263,\n",
       " 'bacalhau': 406,\n",
       " 'maravilhosa': 2628,\n",
       " 'saladona': 3811,\n",
       " 'opcao': 3017,\n",
       " 'por': 3340,\n",
       " 'reais': 3588,\n",
       " 'no': 2930,\n",
       " 'almoco': 190,\n",
       " 'servico': 3927,\n",
       " 'gentil': 2030,\n",
       " 'unico': 4352,\n",
       " 'problema': 3446,\n",
       " 'demora': 1253,\n",
       " 'desnecessario': 1302,\n",
       " 'colocar': 926,\n",
       " 'todos': 4235,\n",
       " 'os': 3039,\n",
       " 'funcionarios': 1974,\n",
       " 'trabalhar': 4276,\n",
       " 'dia': 1336,\n",
       " 'deveriam': 1329,\n",
       " 'ter': 4176,\n",
       " 'dado': 1176,\n",
       " 'folga': 1881,\n",
       " 'pois': 3325,\n",
       " 'padaria': 3068,\n",
       " 'estava': 1656,\n",
       " 'pouco': 3365,\n",
       " 'movimento': 2830,\n",
       " 'fica': 1828,\n",
       " 'dica': 1341,\n",
       " 'pro': 3445,\n",
       " '0000': 2,\n",
       " 'pessimo': 3251,\n",
       " 'custo': 1174,\n",
       " 'beneficio': 501,\n",
       " 'rodizio': 3729,\n",
       " 'menu': 2701,\n",
       " 'qualidade': 3512,\n",
       " 'do': 1405,\n",
       " 'peixe': 3185,\n",
       " 'baixissima': 427,\n",
       " 'pena': 3193,\n",
       " 'bons': 555,\n",
       " 'cortes': 1102,\n",
       " 'grelhados': 2106,\n",
       " 'especialmente': 1605,\n",
       " 'galeto': 1988,\n",
       " 'buffet': 602,\n",
       " 'saladas': 3809,\n",
       " 'bom': 547,\n",
       " 'destaque': 1316,\n",
       " 'legumes': 2467,\n",
       " 'atendimento': 369,\n",
       " 'rapido': 3575,\n",
       " 'pratos': 3381,\n",
       " 'idem': 2212,\n",
       " 'preco': 3391,\n",
       " 'mais': 2597,\n",
       " 'pela': 3188,\n",
       " 'deliciosa': 1244,\n",
       " 'vezes': 4448,\n",
       " 'desanimo': 1269,\n",
       " 'tentar': 4172,\n",
       " 'comer': 954,\n",
       " 'tomar': 4242,\n",
       " 'cafe': 634,\n",
       " 'enchecao': 1499,\n",
       " 'saco': 3793,\n",
       " 'sentar': 3903,\n",
       " 'nem': 2909,\n",
       " 'organizar': 3034,\n",
       " 'uma': 4344,\n",
       " 'fila': 1842,\n",
       " 'faz': 1791,\n",
       " 'cada': 632,\n",
       " 'um': 4343,\n",
       " 'olho': 3003,\n",
       " 'senta': 3898,\n",
       " 'quem': 3533,\n",
       " 'conseguir': 1042,\n",
       " 'alcancar': 159,\n",
       " 'mesa': 2711,\n",
       " 'primeiro': 3437,\n",
       " 'calor': 667,\n",
       " 'maior': 2593,\n",
       " 'forno': 1902,\n",
       " 'pizza': 3296,\n",
       " 'sair': 3802,\n",
       " 'horas': 2182,\n",
       " 'crepes': 1131,\n",
       " 'justo': 2404,\n",
       " 'lanches': 2452,\n",
       " 'diversas': 1387,\n",
       " 'combinacoes': 933,\n",
       " 'pequeno': 3202,\n",
       " 'tem': 4152,\n",
       " 'mural': 2856,\n",
       " 'pra': 3374,\n",
       " 'deixar': 1224,\n",
       " 'sua': 4047,\n",
       " 'marca': 2633,\n",
       " 'cartao': 728,\n",
       " 'fidelidade': 1835,\n",
       " 'falafel': 1755,\n",
       " 'delicioso': 1247,\n",
       " 'porcao': 3341,\n",
       " 'ou': 3046,\n",
       " 'sanduiche': 3831,\n",
       " 'lanche': 2451,\n",
       " 'vegano': 4400,\n",
       " 'so': 3983,\n",
       " 'pedir': 3177,\n",
       " 'sem': 3880,\n",
       " 'queijos': 3528,\n",
       " 'tambem': 4122,\n",
       " 'otimo': 3044,\n",
       " 'se': 3855,\n",
       " 'chegar': 818,\n",
       " 'almocar': 188,\n",
       " 'cedo': 760,\n",
       " 'consegue': 1039,\n",
       " 'lugar': 2567,\n",
       " 'boa': 530,\n",
       " 'espetinhos': 1622,\n",
       " 'tenham': 4167,\n",
       " 'gosto': 2067,\n",
       " 'tempero': 4161,\n",
       " 'carne': 716,\n",
       " 'baita': 424,\n",
       " 'japa': 2369,\n",
       " 'demorado': 1255,\n",
       " 'atendentes': 362,\n",
       " 'mal': 2598,\n",
       " 'informados': 2292,\n",
       " 'kilo': 2433,\n",
       " 'melhor': 2683,\n",
       " 'ambiente': 210,\n",
       " 'externo': 1735,\n",
       " 'domingo': 1419,\n",
       " 'legal': 2466,\n",
       " 'proposta': 3468,\n",
       " 'salada': 3808,\n",
       " 'na': 2873,\n",
       " 'berrini': 509,\n",
       " 'necessidade': 2899,\n",
       " 'gostei': 2065,\n",
       " 'gostoso': 2073,\n",
       " 'segunda': 3869,\n",
       " 'feira': 1807,\n",
       " 'peixes': 3186,\n",
       " 'frescos': 1942,\n",
       " 'guioza': 2130,\n",
       " 'salmao': 3817,\n",
       " 'salgado': 3815,\n",
       " 'sao': 3835,\n",
       " 'paulo': 3156,\n",
       " 'matar': 2653,\n",
       " 'vontade': 4508,\n",
       " 'sushi': 4099,\n",
       " 'durante': 1443,\n",
       " 'semana': 3881,\n",
       " 'misso': 2760,\n",
       " 'gratis': 2095,\n",
       " 'tao': 4131,\n",
       " 'cobrando': 901,\n",
       " 'banana': 437,\n",
       " 'caramelada': 700,\n",
       " 'das': 1188,\n",
       " 'carnes': 718,\n",
       " 'costelas': 1105,\n",
       " 'suinas': 4071,\n",
       " 'limpeza': 2518,\n",
       " 'familiar': 1767,\n",
       " 'deixa': 1220,\n",
       " 'desejar': 1292,\n",
       " 'excelente': 1695,\n",
       " 'voltar': 4501,\n",
       " 'arroz': 322,\n",
       " 'pato': 3152,\n",
       " 'divino': 1398,\n",
       " 'dos': 1430,\n",
       " 'meus': 2723,\n",
       " 'lugares': 2568,\n",
       " 'preferidos': 3403,\n",
       " 'liberdade': 2500,\n",
       " 'tanto': 4129,\n",
       " 'salgados': 3816,\n",
       " 'qto': 3507,\n",
       " 'doces': 1410,\n",
       " 'fantasticos': 1774,\n",
       " 'hachiberry': 2141,\n",
       " 'delicia': 1239,\n",
       " 'eu': 1679,\n",
       " 'adoro': 122,\n",
       " 'bombom': 550,\n",
       " 'peca': 3160,\n",
       " 'mini': 2750,\n",
       " 'for': 1892,\n",
       " 'sobremesa': 3987,\n",
       " 'tiver': 4220,\n",
       " 'estomago': 1664,\n",
       " 'hehe': 2160,\n",
       " 'taca': 4108,\n",
       " 'colegial': 920,\n",
       " 'perfeita': 3218,\n",
       " 'batata': 470,\n",
       " 'rustica': 3769,\n",
       " 'oferecer': 2986,\n",
       " 'como': 966,\n",
       " 'cortesia': 1103,\n",
       " 'seria': 3919,\n",
       " 'simpatico': 3968,\n",
       " 'pisco': 3293,\n",
       " 'sour': 4024,\n",
       " 'simplemente': 3970,\n",
       " 'sensacional': 3894,\n",
       " 'polvo': 3331,\n",
       " 'crocante': 1140,\n",
       " 'entrada': 1535,\n",
       " 'inesperada': 2284,\n",
       " 'surpresa': 4096,\n",
       " 'imediatamente': 2230,\n",
       " 'levou': 2493,\n",
       " 'minha': 2748,\n",
       " 'avaliacao': 395,\n",
       " 'restaurante': 3694,\n",
       " 'top': 4253,\n",
       " 'da': 1175,\n",
       " 'lista': 2529,\n",
       " '000': 1,\n",
       " 'peruano': 3240,\n",
       " 'costela': 1104,\n",
       " 'bafo': 416,\n",
       " 'mandioca': 2605,\n",
       " 'frita': 1953,\n",
       " 'maravilha': 2626,\n",
       " 'recomendo': 3616,\n",
       " 'kebabes': 2422,\n",
       " 'aqui': 301,\n",
       " 'vc': 4398,\n",
       " 'encontra': 1502,\n",
       " 'variedade': 4391,\n",
       " 'enorme': 1520,\n",
       " 'sabores': 3784,\n",
       " 'philly': 3266,\n",
       " 'steak': 4038,\n",
       " 'menos': 2697,\n",
       " 'queijo': 3527,\n",
       " 'ementhal': 1481,\n",
       " 'amostra': 227,\n",
       " 'pq': 3373,\n",
       " 'sentir': 3908,\n",
       " 'pao': 3099,\n",
       " 'murcho': 2857,\n",
       " 'quantidade': 3515,\n",
       " 'cebola': 757,\n",
       " 'pequena': 3200,\n",
       " 'resumindo': 3699,\n",
       " 'decepcao': 1197,\n",
       " 'total': 4271,\n",
       " 'otima': 3042,\n",
       " 'gosta': 2061,\n",
       " 'japonesa': 2373,\n",
       " 'quer': 3536,\n",
       " 'gastar': 2008,\n",
       " 'vila': 4461,\n",
       " 'olimpia': 3006,\n",
       " 'poderiam': 3320,\n",
       " 'caprichar': 694,\n",
       " 'molhinho': 2788,\n",
       " 'alguns': 178,\n",
       " 'nos': 2946,\n",
       " 'servirmos': 3936,\n",
       " 'hoje': 2173,\n",
       " 'pimentinha': 3280,\n",
       " 'calabresa': 654,\n",
       " 'ficou': 1834,\n",
       " 'faltando': 1762,\n",
       " 'kebab': 2421,\n",
       " 'frango': 1923,\n",
       " 'curry': 1168,\n",
       " 'sugere': 4063,\n",
       " 'informalidade': 2294,\n",
       " 'etc': 1677,\n",
       " 'precos': 3393,\n",
       " 'gourmetizados': 2079,\n",
       " 'limonada': 2515,\n",
       " 'colorida': 928,\n",
       " 'nda': 2896,\n",
       " 'demais': 1251,\n",
       " 'maionese': 2592,\n",
       " 'casa': 735,\n",
       " 'pode': 3316,\n",
       " 'serve': 3923,\n",
       " 'duas': 1439,\n",
       " 'pessoas': 3255,\n",
       " 'hamburguer': 2147,\n",
       " 'tropicalia': 4310,\n",
       " 'fantastico': 1773,\n",
       " 'conferir': 1018,\n",
       " 'ruinzinho': 3765,\n",
       " 'bisteca': 519,\n",
       " 'aceitar': 53,\n",
       " 'credito': 1123,\n",
       " 'american': 217,\n",
       " 'express': 1730,\n",
       " 'centro': 767,\n",
       " 'contrasenso': 1064,\n",
       " 'porcoes': 3343,\n",
       " 'servidas': 3931,\n",
       " 'va': 4363,\n",
       " 'la': 2442,\n",
       " 'sozinho': 4027,\n",
       " 'geral': 2033,\n",
       " 'bureka': 606,\n",
       " 'quanto': 3516,\n",
       " 'falaram': 1759,\n",
       " 'podia': 3321,\n",
       " 'ser': 3916,\n",
       " 'maiorzinho': 2596,\n",
       " 'quase': 3520,\n",
       " 'gnocchi': 2045,\n",
       " 'mandioquinha': 2606,\n",
       " 'ao': 261,\n",
       " 'ragu': 3564,\n",
       " 'mignon': 2737,\n",
       " 'mix': 2768,\n",
       " 'cogumelos': 913,\n",
       " 'harmonizado': 2154,\n",
       " 'montes': 2798,\n",
       " 'reserva': 3680,\n",
       " 'cozinheiro': 1119,\n",
       " 'relaxado': 3662,\n",
       " 'pedi': 3170,\n",
       " 'burrito': 615,\n",
       " 'vegetariano': 4404,\n",
       " 'veio': 4408,\n",
       " 'moida': 2783,\n",
       " 'fui': 1968,\n",
       " 'reclamar': 3605,\n",
       " 'disse': 1381,\n",
       " 'ue': 4334,\n",
       " 'pedido': 3173,\n",
       " 'ta': 4105,\n",
       " 'certo': 774,\n",
       " 'caiu': 647,\n",
       " 'meio': 2680,\n",
       " 'nunca': 2967,\n",
       " 'contemporaneo': 1058,\n",
       " 'show': 3954,\n",
       " 'ceviche': 780,\n",
       " 'pescado': 3246,\n",
       " 'fresco': 1941,\n",
       " 'recomendado': 3610,\n",
       " 'japones': 2372,\n",
       " 'tradicional': 4281,\n",
       " 'dentro': 1263,\n",
       " 'media': 2664,\n",
       " 'mto': 2837,\n",
       " 'feitos': 1812,\n",
       " 'servidos': 3933,\n",
       " 'esta': 1646,\n",
       " 'cheio': 826,\n",
       " 'morango': 2806,\n",
       " 'carnita': 719,\n",
       " 'loca': 2538,\n",
       " 'violento': 4470,\n",
       " 'queima': 3529,\n",
       " 'ate': 352,\n",
       " 'alma': 185,\n",
       " 'espaco': 1594,\n",
       " 'ruim': 3763,\n",
       " 'musica': 2859,\n",
       " 'pessima': 3250,\n",
       " 'compensa': 979,\n",
       " 'toda': 4232,\n",
       " 'qualquer': 3513,\n",
       " 'coisa': 914,\n",
       " 'file': 1844,\n",
       " 'parmegiana': 3124,\n",
       " 'gigante': 2041,\n",
       " 'maravilhoso': 2630,\n",
       " 'ar': 302,\n",
       " 'condicionado': 1012,\n",
       " 'quebrado': 3526,\n",
       " 'sistema': 3978,\n",
       " 'fora': 1893,\n",
       " 'tempo': 4163,\n",
       " 'todo': 4234,\n",
       " 'horrivel': 2185,\n",
       " 'indico': 2278,\n",
       " 'principalmente': 3442,\n",
       " 'varanda': 4381,\n",
       " 'pouca': 3363,\n",
       " 'suco': 4057,\n",
       " 'verde': 4435,\n",
       " 'bastante': 467,\n",
       " 'gengibre': 2025,\n",
       " 'lado': 2445,\n",
       " 'pure': 3496,\n",
       " 'parecia': 3115,\n",
       " 'provo': 3483,\n",
       " 'depois': 1265,\n",
       " 'atencioso': 356,\n",
       " 'correram': 1097,\n",
       " 'quarteirao': 3519,\n",
       " 'me': 2663,\n",
       " 'devolver': 1333,\n",
       " 'casaco': 736,\n",
       " 'esqueci': 1636,\n",
       " 'grande': 2085,\n",
       " 'estacionamento': 1649,\n",
       " 'proprio': 3471,\n",
       " 'outro': 3052,\n",
       " 'rua': 3761,\n",
       " 'chic': 830,\n",
       " 'cobra': 898,\n",
       " 'bauru': 479,\n",
       " 'acompanhamentos': 84,\n",
       " 'coca': 904,\n",
       " 'nhoque': 2921,\n",
       " 'rotisseria': 3751,\n",
       " 'gelados': 2018,\n",
       " 'bolos': 543,\n",
       " 'divinos': 1399,\n",
       " 'poucos': 3366,\n",
       " 'onde': 3008,\n",
       " 'paes': 3073,\n",
       " 'integrais': 2315,\n",
       " 'verdade': 4432,\n",
       " 'gostosos': 2074,\n",
       " 'alem': 166,\n",
       " 'primeira': 3436,\n",
       " 'tudo': 4324,\n",
       " 'itaim': 2355,\n",
       " 'isso': 2353,\n",
       " 'alto': 199,\n",
       " 'claro': 883,\n",
       " 'local': 2539,\n",
       " 'relacao': 3660,\n",
       " 'maluka': 2600,\n",
       " 'cardapio': 707,\n",
       " 'impecavel': 2235,\n",
       " 'trilha': 4299,\n",
       " 'sonora': 4010,\n",
       " 'incrivel': 2270,\n",
       " 'aos': 263,\n",
       " 'finais': 1852,\n",
       " 'provar': 3476,\n",
       " 'mussels': 2865,\n",
       " 'thai': 4199,\n",
       " 'mexilhoes': 2733,\n",
       " 'leite': 2469,\n",
       " 'coco': 907,\n",
       " 'balada': 431,\n",
       " 'garcons': 2001,\n",
       " 'final': 1853,\n",
       " 'noite': 2932,\n",
       " 'pior': 3285,\n",
       " 'ainda': 149,\n",
       " 'confuso': 1025,\n",
       " 'as': 331,\n",
       " 'tortas': 4264,\n",
       " 'boas': 532,\n",
       " 'granola': 2087,\n",
       " 'salgada': 3813,\n",
       " 'vem': 4416,\n",
       " 'previam': 3434,\n",
       " 'resolver': 3687,\n",
       " 'urgentemente': 4357,\n",
       " 'questao': 3544,\n",
       " 'curiosamente': 1166,\n",
       " 'baratos': 449,\n",
       " 'sucos': 4058,\n",
       " 'caros': 724,\n",
       " 'tomates': 4245,\n",
       " 'verdes': 4436,\n",
       " 'fritos': 1957,\n",
       " 'devil': 1331,\n",
       " 'cake': 653,\n",
       " 'dividir': 1395,\n",
       " 'ja': 2361,\n",
       " 'sim': 3964,\n",
       " 'quiser': 3559,\n",
       " 'rico': 3713,\n",
       " 'detalhes': 1320,\n",
       " 'aconchegante': 87,\n",
       " 'jantar': 2368,\n",
       " 'amigos': 224,\n",
       " 'veggie': 4406,\n",
       " 'burguer': 610,\n",
       " 'milkshake': 2742,\n",
       " 'avela': 397,\n",
       " 'vidaaaa': 4453,\n",
       " 'onion': 3012,\n",
       " 'rings': 3721,\n",
       " 'atendida': 365,\n",
       " 'apresentacao': 287,\n",
       " 'risotos': 3723,\n",
       " 'elaborados': 1466,\n",
       " 'simples': 3971,\n",
       " 'agradavel': 136,\n",
       " 'volto': 4505,\n",
       " 'massas': 2649,\n",
       " 'cubana': 1151,\n",
       " 'bife': 514,\n",
       " 'milanesa': 2740,\n",
       " 'servido': 3932,\n",
       " 'achado': 66,\n",
       " 'sp': 4028,\n",
       " 'expresso': 1731,\n",
       " 'fraco': 1913,\n",
       " 'conjunto': 1036,\n",
       " 'pastel': 3149,\n",
       " 'dias': 1340,\n",
       " 'raiva': 3565,\n",
       " 'bela': 493,\n",
       " 'carta': 727,\n",
       " 'vinhos': 4468,\n",
       " 'diversificada': 1389,\n",
       " 'linguica': 2526,\n",
       " 'artesanal': 329,\n",
       " 'javali': 2381,\n",
       " 'entrecote': 1542,\n",
       " 'termine': 4183,\n",
       " 'churros': 865,\n",
       " 'caseiro': 745,\n",
       " 'pedida': 3171,\n",
       " 'bacana': 408,\n",
       " 'tranquilo': 4285,\n",
       " 'mesinhas': 2713,\n",
       " 'java': 2380,\n",
       " 'chip': 843,\n",
       " 'assuste': 350,\n",
       " 'longas': 2551,\n",
       " 'esperas': 1612,\n",
       " 'drinks': 1436,\n",
       " 'entradas': 1536,\n",
       " 'otimas': 3043,\n",
       " 'deixe': 1226,\n",
       " 'sanduba': 3830,\n",
       " 'gyoza': 2132,\n",
       " 'balcao': 434,\n",
       " 'perguntar': 3226,\n",
       " 'qual': 3510,\n",
       " 'ramen': 3567,\n",
       " 'mesmo': 2715,\n",
       " 'tangerina': 4126,\n",
       " 'costuma': 1108,\n",
       " 'aguado': 142,\n",
       " 'opcoes': 3018,\n",
       " 'acai': 49,\n",
       " 'imitacao': 2233,\n",
       " 'don': 1422,\n",
       " 'miguel': 2738,\n",
       " 'esquema': 1638,\n",
       " 'outback': 3049,\n",
       " 'comeco': 948,\n",
       " 'vai': 4366,\n",
       " 'melhorando': 2685,\n",
       " 'estao': 1653,\n",
       " 'idade': 2210,\n",
       " 'pedranestes': 3179,\n",
       " 'tempos': 4164,\n",
       " 'assaltam': 340,\n",
       " 'cego': 761,\n",
       " 'andar': 234,\n",
       " 'dinheiro': 1365,\n",
       " 'fazer': 1795,\n",
       " 'minimo': 2754,\n",
       " 'estupidez': 1675,\n",
       " 'farta': 1776,\n",
       " 'diferente': 1348,\n",
       " 'convencional': 1066,\n",
       " 'paulistano': 3155,\n",
       " 'paladar': 3084,\n",
       " 'ocidental': 2979,\n",
       " 'torcer': 4258,\n",
       " 'nariz': 2886,\n",
       " 'fritas': 1955,\n",
       " 'eh': 1461,\n",
       " 'feijao': 1803,\n",
       " 'wifi': 4521,\n",
       " 'nbsteak00': 2895,\n",
       " 'rigatonni': 3719,\n",
       " 'amatricciana': 209,\n",
       " 'apimentado': 283,\n",
       " 'cafes': 635,\n",
       " 'chocolate': 849,\n",
       " 'fortemente': 1904,\n",
       " 'recomendados': 3611,\n",
       " 'acabei': 46,\n",
       " 'vida': 4452,\n",
       " 'polloloco': 3329,\n",
       " 'comi': 959,\n",
       " 'outros': 3053,\n",
       " 'aki': 153,\n",
       " 'mexicana': 2725,\n",
       " 'agora': 134,\n",
       " 'dar': 1187,\n",
       " 'moderninho': 2778,\n",
       " 'buena': 598,\n",
       " 'suerte': 4060,\n",
       " 'cabron': 625,\n",
       " 'atum': 382,\n",
       " 'esse': 1642,\n",
       " 'acessivel': 64,\n",
       " 'rotatividade': 3750,\n",
       " 'sashimis': 3841,\n",
       " 'nota': 2950,\n",
       " 'servida': 3930,\n",
       " 'temperada': 4157,\n",
       " 'porem': 3344,\n",
       " 'vindo': 4465,\n",
       " 'rong': 3740,\n",
       " 'he': 2159,\n",
       " 'tutoia': 4330,\n",
       " 'estacionar': 1651,\n",
       " 'perto': 3237,\n",
       " 'vegetariana': 4402,\n",
       " 'precisa': 3386,\n",
       " 'graca': 2082,\n",
       " 'soja': 3999,\n",
       " 'mim': 2744,\n",
       " 'goshala': 2060,\n",
       " 'pinheiros': 3283,\n",
       " 'amo': 225,\n",
       " 'vou': 4509,\n",
       " 'sugiro': 4066,\n",
       " 'chai': 784,\n",
       " 'latte': 2461,\n",
       " 'cinnamon': 876,\n",
       " 'roll': 3734,\n",
       " 'evitar': 1686,\n",
       " 'horario': 2180,\n",
       " 'saida': 3798,\n",
       " 'alunos': 202,\n",
       " 'escolas': 1572,\n",
       " 'entorno': 1533,\n",
       " 'loja': 2544,\n",
       " '00h': 5,\n",
       " 'nesse': 2914,\n",
       " 'ficar': 1831,\n",
       " 'interno': 2325,\n",
       " 'bolinho': 539,\n",
       " 'penne': 3195,\n",
       " 'mediterraneo': 2672,\n",
       " 'risotto': 3724,\n",
       " 'aipo': 150,\n",
       " 'excelentes': 1697,\n",
       " 'caprichado': 692,\n",
       " 'combinados': 936,\n",
       " 'teppan': 4175,\n",
       " 'yaki': 4536,\n",
       " 'executivo': 1707,\n",
       " 'teishoko': 4147,\n",
       " 'ume': 4346,\n",
       " 'tempura': 4165,\n",
       " 'anchova': 231,\n",
       " 'grelhada': 2104,\n",
       " 'placa': 3300,\n",
       " 'grata': 2092,\n",
       " 'frequentado': 1936,\n",
       " 'inca': 2256,\n",
       " 'kola': 2438,\n",
       " 'cerveja': 776,\n",
       " 'chocolates': 850,\n",
       " 'peruanos': 3241,\n",
       " 'adivinha': 112,\n",
       " 'picanha': 3270,\n",
       " 'fatiada': 1783,\n",
       " 'polenta': 3327,\n",
       " 'analisar': 229,\n",
       " 'contexto': 1060,\n",
       " 'vir': 4472,\n",
       " 'agil': 132,\n",
       " 'camarao': 671,\n",
       " 'mundo': 2853,\n",
       " 'impossivel': 2242,\n",
       " 'super': 4083,\n",
       " 'mega': 2676,\n",
       " 'recheado': 3598,\n",
       " 'compro': 1000,\n",
       " 'viagem': 4450,\n",
       " 'nas': 2888,\n",
       " 'mesas': 2712,\n",
       " 'minutos': 2757,\n",
       " 'strogonoff': 4045,\n",
       " 'cogumelo': 912,\n",
       " 'paris': 3122,\n",
       " 'pessoa': 3253,\n",
       " 'to': 4223,\n",
       " 'cansada': 680,\n",
       " 'cara': 699,\n",
       " 'porta': 3349,\n",
       " 'ultimas': 4339,\n",
       " 'fechado': 1798,\n",
       " 'ora': 3024,\n",
       " 'reforma': 3640,\n",
       " 'feriado': 1818,\n",
       " 'ferias': 1819,\n",
       " 'coletivas': 922,\n",
       " 'abre': 33,\n",
       " 'horarios': 2181,\n",
       " 'limitados': 2513,\n",
       " 'levam': 2485,\n",
       " 'serio': 3921,\n",
       " 'concorrente': 1007,\n",
       " 'parte': 3127,\n",
       " 'tres': 4298,\n",
       " 'cupuacu': 1164,\n",
       " 'simplesmente': 3972,\n",
       " 'galeria': 1986,\n",
       " 'espaҫo': 1599,\n",
       " 'talher': 4116,\n",
       " 'sobremesas': 3988,\n",
       " 'telao': 4150,\n",
       " 'propaganda': 3466,\n",
       " 'coberto': 896,\n",
       " 'vi': 4449,\n",
       " 'espetos': 1624,\n",
       " 'maquina': 2621,\n",
       " 'sendo': 3886,\n",
       " 'lavados': 2462,\n",
       " 'chao': 795,\n",
       " 'consegui': 1041,\n",
       " 'contar': 1056,\n",
       " 'banheiro': 443,\n",
       " 'imundo': 2250,\n",
       " 'praticamente': 3378,\n",
       " 'paella': 3072,\n",
       " 'acafrao': 48,\n",
       " 'assim': 343,\n",
       " 'seca': 3857,\n",
       " 'lento': 2478,\n",
       " 'tiverem': 4221,\n",
       " 'filial': 1847,\n",
       " 'ceviches': 781,\n",
       " 'descontraidos': 1284,\n",
       " 'experimente': 1723,\n",
       " 'ravioli': 3583,\n",
       " 'brie': 581,\n",
       " 'figo': 1840,\n",
       " 'incriveis': 2269,\n",
       " 'dessa': 1309,\n",
       " 'saraiva': 3839,\n",
       " 'megastore': 2677,\n",
       " 'entao': 1527,\n",
       " 'voce': 4495,\n",
       " 'seu': 3937,\n",
       " 'enquanto': 1523,\n",
       " 'escolhe': 1574,\n",
       " 'livro': 2535,\n",
       " 'comprar': 995,\n",
       " 'le': 2464,\n",
       " 'lo': 2537,\n",
       " 'feliz': 1815,\n",
       " 'starbucks': 4036,\n",
       " 'pop': 3337,\n",
       " 'dizer': 1404,\n",
       " 'cha': 782,\n",
       " 'eles': 1472,\n",
       " 'cobram': 900,\n",
       " 'lotado': 2560,\n",
       " 'senha': 3887,\n",
       " '0000000000': 3,\n",
       " 'parede': 3117,\n",
       " 'otimos': 3045,\n",
       " 'caldos': 660,\n",
       " 'decepcionou': 1203,\n",
       " 'kebabs': 2423,\n",
       " 'estavam': 1657,\n",
       " 'quando': 3514,\n",
       " 'falam': 1756,\n",
       " 'significa': 3962,\n",
       " 'homus': 2175,\n",
       " 'apenas': 277,\n",
       " 'ok': 2994,\n",
       " 'kibe': 2430,\n",
       " 'assado': 338,\n",
       " 'era': 1556,\n",
       " 'charutinho': 802,\n",
       " 'uva': 4362,\n",
       " 'tinha': 4202,\n",
       " 'feno': 1816,\n",
       " 'antes': 254,\n",
       " 'esperando': 1608,\n",
       " 'filas': 1843,\n",
       " 'ficam': 1829,\n",
       " 'enormes': 1521,\n",
       " 'farto': 1778,\n",
       " 'deliciosas': 1246,\n",
       " 'cone': 1016,\n",
       " 'cordeiro': 1084,\n",
       " 'quinta': 3554,\n",
       " 'combo': 939,\n",
       " 'promocional': 3460,\n",
       " 'acompanhamento': 83,\n",
       " 'refrigerante': 3648,\n",
       " 'pecam': 3162,\n",
       " 'catupiry': 753,\n",
       " 'experimentem': 1725,\n",
       " 'deliciosos': 1248,\n",
       " 'coxinha': 1113,\n",
       " 'esquecam': 1632,\n",
       " 'adicionar': 109,\n",
       " 'especial': 1601,\n",
       " 'estiver': 1663,\n",
       " 'fome': 1887,\n",
       " 'acima': 74,\n",
       " 'razoavel': 3587,\n",
       " 'combos': 940,\n",
       " 'valem': 4368,\n",
       " 'deve': 1326,\n",
       " 'sido': 3960,\n",
       " 'algo': 172,\n",
       " 'barulhento': 461,\n",
       " 'momento': 2792,\n",
       " 'flamenco': 1868,\n",
       " 'seguranca': 3872,\n",
       " 'visitei': 4481,\n",
       " 'tenha': 4166,\n",
       " 'mostrado': 2820,\n",
       " 'desinformado': 1297,\n",
       " 'lula': 2570,\n",
       " 'dore': 1428,\n",
       " 'pequenas': 3201,\n",
       " 'altissimo': 198,\n",
       " 'frio': 1949,\n",
       " 'convidativo': 1074,\n",
       " 'doce': 1408,\n",
       " 'ambos': 211,\n",
       " 'grandes': 2086,\n",
       " 'recheados': 3599,\n",
       " 'espera': 1607,\n",
       " 'demorada': 1254,\n",
       " 'mercadao': 2702,\n",
       " 'organizacao': 3030,\n",
       " 'diminuicao': 1359,\n",
       " 'tornou': 4261,\n",
       " 'hocca': 2172,\n",
       " 'muitooo': 2848,\n",
       " 'rs': 3755,\n",
       " 'simpaticos': 3969,\n",
       " 'crepe': 1129,\n",
       " 'nuttela': 2969,\n",
       " 'cartaozinho': 729,\n",
       " 'con': 1002,\n",
       " 'carimbos': 710,\n",
       " 'troco': 4308,\n",
       " 'maravilhaaa': 2627,\n",
       " 'atendido': 367,\n",
       " 'garcom': 1999,\n",
       " 'francke': 1921,\n",
       " 'outra': 3050,\n",
       " 'menina': 2693,\n",
       " 'burger': 608,\n",
       " 'quente': 3534,\n",
       " 'quindim': 3552,\n",
       " 'sigam': 3961,\n",
       " 'twitter': 4332,\n",
       " 'facebook': 1746,\n",
       " 'pizzaria': 3297,\n",
       " 'regiao': 3651,\n",
       " 'ha': 2136,\n",
       " 'maioria': 2595,\n",
       " 'melhores': 2688,\n",
       " 'docinhos': 1413,\n",
       " 'detox': 1323,\n",
       " 'donuts': 1427,\n",
       " 'sagadinhos': 3796,\n",
       " 'ver': 4429,\n",
       " 'frente': 1935,\n",
       " 'prestigiado': 3430,\n",
       " 'alex': 169,\n",
       " 'atala': 351,\n",
       " 'tenho': 4169,\n",
       " 'dificuldade': 1351,\n",
       " 'escolher': 1576,\n",
       " 'parecem': 3114,\n",
       " 'terceira': 4179,\n",
       " 'visita': 4478,\n",
       " 'tristemente': 4303,\n",
       " 'arais': 304,\n",
       " 'diminuido': 1360,\n",
       " 'massa': 2648,\n",
       " 'sirio': 3977,\n",
       " 'industrializada': 2283,\n",
       " 'rap00': 3570,\n",
       " 'mesma': 2714,\n",
       " 'famoso': 1770,\n",
       " 'jeito': 2382,\n",
       " 'focar': 1874,\n",
       " 'limpinho': 2519,\n",
       " 'recinto': 3602,\n",
       " 'sujo': 4077,\n",
       " 'manutencao': 2616,\n",
       " 'conter': 1059,\n",
       " 'briga': 582,\n",
       " 'ocorreu': 2980,\n",
       " 'deixou': 1229,\n",
       " 'essa': 1640,\n",
       " 'churrascaria': 862,\n",
       " 'vive': 4487,\n",
       " 'nome': 2934,\n",
       " 'ultrapassada': 4342,\n",
       " 'outras': 3051,\n",
       " 'fama': 1765,\n",
       " 'impressionar': 2246,\n",
       " 'turistas': 4329,\n",
       " 'bar': 446,\n",
       " 'descolado': 1278,\n",
       " 'mesmos': 2716,\n",
       " 'donos': 1426,\n",
       " 'myk': 2872,\n",
       " 'grega': 2101,\n",
       " 'compensado': 980,\n",
       " 'vinho': 4467,\n",
       " 'grego': 2102,\n",
       " 'clericot': 886,\n",
       " 'evite': 1687,\n",
       " 'carro': 726,\n",
       " 'dificil': 1350,\n",
       " 'achar': 69,\n",
       " 'vaga': 4364,\n",
       " 'hora': 2179,\n",
       " 'cervejas': 777,\n",
       " 'artesanais': 328,\n",
       " 'chopps': 857,\n",
       " 'comidinhas': 963,\n",
       " 'plantas': 3306,\n",
       " 'voltam': 4500,\n",
       " 'criam': 1134,\n",
       " 'guiosa': 2129,\n",
       " 'divina': 1396,\n",
       " 'fecha': 1797,\n",
       " 'poucas': 3364,\n",
       " 'lindo': 2523,\n",
       " 'verdadeiro': 4434,\n",
       " 'jardim': 2376,\n",
       " 'excessos': 1702,\n",
       " 'acredite': 94,\n",
       " 'acessiveis': 63,\n",
       " 'promocoes': 3461,\n",
       " 'interessantes': 2321,\n",
       " 'poderia': 3319,\n",
       " 'melhorar': 2686,\n",
       " 'self': 3879,\n",
       " 'seja': 3874,\n",
       " 'deveria': 1328,\n",
       " 'experimentar': 1722,\n",
       " 'batera': 475,\n",
       " 'quartas': 3518,\n",
       " 'spaguetti': 4030,\n",
       " 'desossado': 1304,\n",
       " 'filet': 1845,\n",
       " 'sente': 3905,\n",
       " 'marcio': 2635,\n",
       " 'chapa': 796,\n",
       " 'co': 892,\n",
       " 'requeijao': 3673,\n",
       " 'preparado': 3417,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countVec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1723],\n",
       "        [  72],\n",
       "        [ 720]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(countVec.transform(['experimente', 'achei', 'caro']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1542x4551 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 25320 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectTexts_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(vectTexts_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111543450064851"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(vectTexts_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7790697674418605"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(vectTexts_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'alpha': [0.001, 0.1, 1, 10, 100], 'fit_prior': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnbParams = { #'verbose' : [1],\n",
    "             'alpha':[0.001, 0.1,1,10, 100],  \n",
    "             'fit_prior' :[True, False]}\n",
    "mnbRSCV = RandomizedSearchCV(mnb, mnbParams, verbose=1, return_train_score=True) #, n_jobs=-1)\n",
    "mnbRSCV.fit(vectTexts_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_fit_prior</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004665</td>\n",
       "      <td>1.886325e-03</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>4.713704e-04</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'fit_prior': True, 'alpha': 0.001}</td>\n",
       "      <td>0.753398</td>\n",
       "      <td>0.776699</td>\n",
       "      <td>0.757812</td>\n",
       "      <td>0.762646</td>\n",
       "      <td>0.010113</td>\n",
       "      <td>3</td>\n",
       "      <td>0.988315</td>\n",
       "      <td>0.986368</td>\n",
       "      <td>0.990291</td>\n",
       "      <td>0.988325</td>\n",
       "      <td>0.001602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001999</td>\n",
       "      <td>2.973602e-07</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.123916e-07</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'fit_prior': False, 'alpha': 0.001}</td>\n",
       "      <td>0.745631</td>\n",
       "      <td>0.761165</td>\n",
       "      <td>0.757812</td>\n",
       "      <td>0.754864</td>\n",
       "      <td>0.006680</td>\n",
       "      <td>4</td>\n",
       "      <td>0.988315</td>\n",
       "      <td>0.984421</td>\n",
       "      <td>0.988350</td>\n",
       "      <td>0.987029</td>\n",
       "      <td>0.001844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001999</td>\n",
       "      <td>1.946680e-07</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>4.899036e-07</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'fit_prior': True, 'alpha': 0.1}</td>\n",
       "      <td>0.745631</td>\n",
       "      <td>0.763107</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>0.737354</td>\n",
       "      <td>0.025167</td>\n",
       "      <td>5</td>\n",
       "      <td>0.979552</td>\n",
       "      <td>0.980526</td>\n",
       "      <td>0.986408</td>\n",
       "      <td>0.982162</td>\n",
       "      <td>0.003028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002000</td>\n",
       "      <td>2.973602e-07</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>4.710895e-04</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'fit_prior': False, 'alpha': 0.1}</td>\n",
       "      <td>0.702913</td>\n",
       "      <td>0.714563</td>\n",
       "      <td>0.664062</td>\n",
       "      <td>0.693904</td>\n",
       "      <td>0.021572</td>\n",
       "      <td>7</td>\n",
       "      <td>0.981500</td>\n",
       "      <td>0.976631</td>\n",
       "      <td>0.980583</td>\n",
       "      <td>0.979571</td>\n",
       "      <td>0.002112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001999</td>\n",
       "      <td>4.899036e-07</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>4.711456e-04</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>{'fit_prior': True, 'alpha': 1}</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.791016</td>\n",
       "      <td>0.797017</td>\n",
       "      <td>0.004231</td>\n",
       "      <td>1</td>\n",
       "      <td>0.914314</td>\n",
       "      <td>0.906524</td>\n",
       "      <td>0.909709</td>\n",
       "      <td>0.910182</td>\n",
       "      <td>0.003198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001999</td>\n",
       "      <td>2.247832e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>{'fit_prior': False, 'alpha': 1}</td>\n",
       "      <td>0.782524</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.771484</td>\n",
       "      <td>0.784695</td>\n",
       "      <td>0.011737</td>\n",
       "      <td>2</td>\n",
       "      <td>0.929893</td>\n",
       "      <td>0.927945</td>\n",
       "      <td>0.930097</td>\n",
       "      <td>0.929312</td>\n",
       "      <td>0.000970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.002000</td>\n",
       "      <td>2.973602e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>{'fit_prior': True, 'alpha': 10}</td>\n",
       "      <td>0.689320</td>\n",
       "      <td>0.687379</td>\n",
       "      <td>0.693359</td>\n",
       "      <td>0.690013</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>8</td>\n",
       "      <td>0.704966</td>\n",
       "      <td>0.697176</td>\n",
       "      <td>0.692233</td>\n",
       "      <td>0.698125</td>\n",
       "      <td>0.005241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001999</td>\n",
       "      <td>2.973602e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>{'fit_prior': False, 'alpha': 10}</td>\n",
       "      <td>0.702913</td>\n",
       "      <td>0.706796</td>\n",
       "      <td>0.714844</td>\n",
       "      <td>0.708171</td>\n",
       "      <td>0.004965</td>\n",
       "      <td>6</td>\n",
       "      <td>0.740019</td>\n",
       "      <td>0.724440</td>\n",
       "      <td>0.722330</td>\n",
       "      <td>0.728930</td>\n",
       "      <td>0.007889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001999</td>\n",
       "      <td>1.946680e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>{'fit_prior': True, 'alpha': 100}</td>\n",
       "      <td>0.681553</td>\n",
       "      <td>0.681553</td>\n",
       "      <td>0.683594</td>\n",
       "      <td>0.682231</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>10</td>\n",
       "      <td>0.682571</td>\n",
       "      <td>0.682571</td>\n",
       "      <td>0.681553</td>\n",
       "      <td>0.682232</td>\n",
       "      <td>0.000480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001999</td>\n",
       "      <td>2.247832e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>{'fit_prior': False, 'alpha': 100}</td>\n",
       "      <td>0.685437</td>\n",
       "      <td>0.685437</td>\n",
       "      <td>0.689453</td>\n",
       "      <td>0.686770</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>9</td>\n",
       "      <td>0.697176</td>\n",
       "      <td>0.688413</td>\n",
       "      <td>0.689320</td>\n",
       "      <td>0.691636</td>\n",
       "      <td>0.003935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.004665  1.886325e-03         0.001333    4.713704e-04   \n",
       "1       0.001999  2.973602e-07         0.001000    1.123916e-07   \n",
       "2       0.001999  1.946680e-07         0.000999    4.899036e-07   \n",
       "3       0.002000  2.973602e-07         0.000666    4.710895e-04   \n",
       "4       0.001999  4.899036e-07         0.000333    4.711456e-04   \n",
       "5       0.001999  2.247832e-07         0.000000    0.000000e+00   \n",
       "6       0.002000  2.973602e-07         0.000000    0.000000e+00   \n",
       "7       0.001999  2.973602e-07         0.000000    0.000000e+00   \n",
       "8       0.001999  1.946680e-07         0.000000    0.000000e+00   \n",
       "9       0.001999  2.247832e-07         0.000000    0.000000e+00   \n",
       "\n",
       "  param_fit_prior param_alpha                                params  \\\n",
       "0            True       0.001   {'fit_prior': True, 'alpha': 0.001}   \n",
       "1           False       0.001  {'fit_prior': False, 'alpha': 0.001}   \n",
       "2            True         0.1     {'fit_prior': True, 'alpha': 0.1}   \n",
       "3           False         0.1    {'fit_prior': False, 'alpha': 0.1}   \n",
       "4            True           1       {'fit_prior': True, 'alpha': 1}   \n",
       "5           False           1      {'fit_prior': False, 'alpha': 1}   \n",
       "6            True          10      {'fit_prior': True, 'alpha': 10}   \n",
       "7           False          10     {'fit_prior': False, 'alpha': 10}   \n",
       "8            True         100     {'fit_prior': True, 'alpha': 100}   \n",
       "9           False         100    {'fit_prior': False, 'alpha': 100}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "0           0.753398           0.776699           0.757812         0.762646   \n",
       "1           0.745631           0.761165           0.757812         0.754864   \n",
       "2           0.745631           0.763107           0.703125         0.737354   \n",
       "3           0.702913           0.714563           0.664062         0.693904   \n",
       "4           0.800000           0.800000           0.791016         0.797017   \n",
       "5           0.782524           0.800000           0.771484         0.784695   \n",
       "6           0.689320           0.687379           0.693359         0.690013   \n",
       "7           0.702913           0.706796           0.714844         0.708171   \n",
       "8           0.681553           0.681553           0.683594         0.682231   \n",
       "9           0.685437           0.685437           0.689453         0.686770   \n",
       "\n",
       "   std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0        0.010113                3            0.988315            0.986368   \n",
       "1        0.006680                4            0.988315            0.984421   \n",
       "2        0.025167                5            0.979552            0.980526   \n",
       "3        0.021572                7            0.981500            0.976631   \n",
       "4        0.004231                1            0.914314            0.906524   \n",
       "5        0.011737                2            0.929893            0.927945   \n",
       "6        0.002489                8            0.704966            0.697176   \n",
       "7        0.004965                6            0.740019            0.724440   \n",
       "8        0.000961               10            0.682571            0.682571   \n",
       "9        0.001891                9            0.697176            0.688413   \n",
       "\n",
       "   split2_train_score  mean_train_score  std_train_score  \n",
       "0            0.990291          0.988325         0.001602  \n",
       "1            0.988350          0.987029         0.001844  \n",
       "2            0.986408          0.982162         0.003028  \n",
       "3            0.980583          0.979571         0.002112  \n",
       "4            0.909709          0.910182         0.003198  \n",
       "5            0.930097          0.929312         0.000970  \n",
       "6            0.692233          0.698125         0.005241  \n",
       "7            0.722330          0.728930         0.007889  \n",
       "8            0.681553          0.682232         0.000480  \n",
       "9            0.689320          0.691636         0.003935  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(mnbRSCV.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7790697674418605"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnbRSCV.best_estimator_.score(vectTexts_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Embeddings import WordEmbeddingBR, splitWithPunctuation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NILC word embeddings. More available at http://nilc.icmc.usp.br/embeddings\n",
      "glove50 exists. Skipping.\n",
      "cbow50_wang2vec exists. Skipping.\n",
      "cbow50_fasttext exists. Skipping.\n",
      "skip50_word2vec exists. Skipping.\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cbow50_fasttext',\n",
       " 'cbow50_wang2vec',\n",
       " 'glove1000',\n",
       " 'glove50',\n",
       " 'skip50_word2vec',\n",
       " 'skip_s300_word2vec',\n",
       " 'wang2vec_skip600']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordEmbeddingBR.downloadNILCEmbeddings()\n",
    "WordEmbeddingBR.getAvailableEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embedding file: cbow50_wang2vec.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "934967it [00:33, 28225.54it/s]\n"
     ]
    }
   ],
   "source": [
    "wee = WordEmbeddingBR('skip_s300_word2vec')\n",
    "#wee = WordEmbeddingBR('cbow50_wang2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Support Vector Machine...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:   38.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]Fitting Gradient Boosted Tree...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         792.2168           14.58s\n",
      "         2         683.4647           10.70s\n",
      "         3         606.6007            9.33s\n",
      "         4         545.3375            8.73s\n",
      "         5         501.5752            8.23s\n",
      "         6         458.4456            7.95s\n",
      "         7         426.4868            7.70s\n",
      "         8         391.9938            7.50s\n",
      "         9         364.0820            7.37s\n",
      "        10         343.4600            7.22s\n",
      "        20         197.6383            5.96s\n",
      "        30         128.3983            5.12s\n",
      "        40          85.1715            4.34s\n",
      "        50          58.8923            3.61s\n",
      "        60          41.6619            2.91s\n",
      "        70          29.2244            2.22s\n",
      "        80          20.5887            1.54s\n",
      "        90          14.6022            0.87s\n",
      "       100          10.3200            0.20s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         796.3377            6.93s\n",
      "         2         687.9754            7.02s\n",
      "         3         604.6102            7.06s\n",
      "         4         548.1648            7.05s\n",
      "         5         492.2952            6.98s\n",
      "         6         454.6555            6.85s\n",
      "         7         418.0396            6.90s\n",
      "         8         383.7254            7.18s\n",
      "         9         359.6321            7.30s\n",
      "        10         344.6228            7.26s\n",
      "        20         201.9401            6.16s\n",
      "        30         132.0189            5.19s\n",
      "        40          94.1081            4.38s\n",
      "        50          63.5470            3.65s\n",
      "        60          48.3385            2.92s\n",
      "        70          36.3252            2.23s\n",
      "        80          27.3708            1.54s\n",
      "        90          21.5585            0.87s\n",
      "       100          16.2507            0.20s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         789.8230            7.04s\n",
      "         2         671.1412            7.02s\n",
      "         3         593.4722            6.96s\n",
      "         4         529.8016            6.90s\n",
      "         5         479.9982            6.86s\n",
      "         6         436.5265            6.82s\n",
      "         7         397.7426            6.73s\n",
      "         8         370.9016            6.61s\n",
      "         9         343.5413            6.58s\n",
      "        10         319.3427            6.50s\n",
      "        20         185.8924            5.79s\n",
      "        30         122.0876            4.97s\n",
      "        40          82.9788            4.25s\n",
      "        50          59.6656            3.53s\n",
      "        60          42.4768            2.85s\n",
      "        70          31.1108            2.18s\n",
      "        80          23.7289            1.51s\n",
      "        90          17.9748            0.85s\n",
      "       100          13.7353            0.20s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         692.6255           41.74s\n",
      "         2         541.2682           41.61s\n",
      "         3         426.0768           41.60s\n",
      "         4         340.9877           42.19s\n",
      "         5         276.5282           42.22s\n",
      "         6         232.5996           42.32s\n",
      "         7         195.6512           42.02s\n",
      "         8         164.2519           42.05s\n",
      "         9         140.5336           41.93s\n",
      "        10         121.2468           41.75s\n",
      "        20          35.7571           39.39s\n",
      "        30          13.1327           37.43s\n",
      "        40           4.2486           36.22s\n",
      "        50           1.4025           34.88s\n",
      "        60           0.4623           33.49s\n",
      "        70           0.2902           30.07s\n",
      "        80           0.2902           25.38s\n",
      "        90           0.2902           21.72s\n",
      "       100           0.2902           18.79s\n",
      "       200           0.2902            5.50s\n",
      "       300           0.2902            0.97s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         680.4255           41.73s\n",
      "         2         511.9161           43.11s\n",
      "         3         385.0256           44.36s\n",
      "         4         310.5958           43.76s\n",
      "         5         251.8929           43.54s\n",
      "         6         203.9737           43.63s\n",
      "         7         166.6845           43.28s\n",
      "         8         144.5148           42.99s\n",
      "         9         128.2321           42.44s\n",
      "        10         109.7577           42.17s\n",
      "        20          34.6383           39.64s\n",
      "        30          13.5659           38.02s\n",
      "        40           7.0125           36.29s\n",
      "        50           4.5201           34.77s\n",
      "        60           3.6476           33.16s\n",
      "        70           3.2305           31.54s\n",
      "        80           3.0633           29.34s\n",
      "        90           3.0240           26.31s\n",
      "       100           2.9933           23.84s\n",
      "       200           2.8364           10.03s\n",
      "       300           2.7974            2.24s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         657.9182           44.07s\n",
      "         2         483.8189           43.94s\n",
      "         3         373.8052           43.59s\n",
      "         4         288.3109           43.43s\n",
      "         5         235.7902           43.41s\n",
      "         6         193.9629           43.19s\n",
      "         7         157.5006           43.09s\n",
      "         8         136.9321           43.03s\n",
      "         9         118.9629           42.80s\n",
      "        10         106.1399           42.30s\n",
      "        20          30.1943           40.37s\n",
      "        30          11.6661           38.61s\n",
      "        40           5.8300           37.11s\n",
      "        50           4.0140           35.64s\n",
      "        60           3.3462           33.96s\n",
      "        70           3.0793           31.94s\n",
      "        80           3.0198           28.66s\n",
      "        90           2.9959           25.71s\n",
      "       100           2.9745           23.25s\n",
      "       200           2.8247           10.07s\n",
      "       300           2.7883            2.31s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         934.7955           45.86s\n",
      "         2         909.1851           46.48s\n",
      "         3         884.9816           46.72s\n",
      "         4         862.5929           46.50s\n",
      "         5         840.8645           46.98s\n",
      "         6         820.3114           46.70s\n",
      "         7         800.7860           46.42s\n",
      "         8         782.1001           46.18s\n",
      "         9         763.9801           45.96s\n",
      "        10         747.0846           45.76s\n",
      "        20         595.7328           44.53s\n",
      "        30         487.1930           43.53s\n",
      "        40         401.6683           42.52s\n",
      "        50         335.2752           41.34s\n",
      "        60         281.1306           40.18s\n",
      "        70         238.5587           38.89s\n",
      "        80         203.6109           37.62s\n",
      "        90         175.3936           36.36s\n",
      "       100         151.9747           35.07s\n",
      "       200          44.6827           22.08s\n",
      "       300          15.0055            9.11s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         933.6211           45.49s\n",
      "         2         906.3651           47.03s\n",
      "         3         880.7530           48.07s\n",
      "         4         856.3458           47.97s\n",
      "         5         834.3270           47.71s\n",
      "         6         812.6972           47.49s\n",
      "         7         791.6794           47.25s\n",
      "         8         771.0712           47.17s\n",
      "         9         752.0336           46.96s\n",
      "        10         733.2517           46.81s\n",
      "        20         581.6906           45.32s\n",
      "        30         470.4729           44.21s\n",
      "        40         385.3199           43.42s\n",
      "        50         320.5288           42.16s\n",
      "        60         267.9691           41.00s\n",
      "        70         228.3465           39.82s\n",
      "        80         195.5047           38.44s\n",
      "        90         168.7671           37.07s\n",
      "       100         146.8102           35.72s\n",
      "       200          47.5469           22.10s\n",
      "       300          19.7655            9.04s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         936.5265           47.34s\n",
      "         2         909.0430           47.95s\n",
      "         3         880.5986           48.19s\n",
      "         4         853.5494           48.24s\n",
      "         5         828.0677           48.37s\n",
      "         6         803.6615           48.41s\n",
      "         7         781.0749           48.34s\n",
      "         8         759.1708           48.44s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         9         738.2905           48.33s\n",
      "        10         718.4540           48.14s\n",
      "        20         559.0625           46.35s\n",
      "        30         445.5576           44.95s\n",
      "        40         363.7820           43.44s\n",
      "        50         303.3545           41.98s\n",
      "        60         251.8509           40.65s\n",
      "        70         212.4693           39.35s\n",
      "        80         181.2654           38.03s\n",
      "        90         157.3126           36.63s\n",
      "       100         136.3867           35.31s\n",
      "       200          40.5146           22.14s\n",
      "       300          15.8539            9.12s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         494.0948           58.54s\n",
      "         2         282.7857            1.06m\n",
      "         3         164.0005            1.10m\n",
      "         4          96.4842            1.12m\n",
      "         5          60.1979            1.12m\n",
      "         6          38.5751            1.13m\n",
      "         7          23.6158            1.12m\n",
      "         8          14.6580            1.12m\n",
      "         9           9.7703            1.11m\n",
      "        10           6.2275            1.11m\n",
      "        20           0.3044           53.64s\n",
      "        30           0.2796           34.94s\n",
      "        40           0.2796           25.06s\n",
      "        50           0.2796           19.12s\n",
      "        60           0.2796           15.16s\n",
      "        70           0.2796           12.32s\n",
      "        80           0.2796           10.19s\n",
      "        90           0.2796            8.53s\n",
      "       100           0.2796            7.19s\n",
      "       200           0.2796            1.10s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         461.6889            1.03m\n",
      "         2         257.4993            1.09m\n",
      "         3         151.4846            1.11m\n",
      "         4          86.6567            1.13m\n",
      "         5          53.0692            1.13m\n",
      "         6          35.0398            1.12m\n",
      "         7          22.3715            1.12m\n",
      "         8          15.7088            1.11m\n",
      "         9          11.3808            1.10m\n",
      "        10           8.5682            1.09m\n",
      "        20           3.0968           55.28s\n",
      "        30           3.0136           39.53s\n",
      "        40           2.9489           31.16s\n",
      "        50           2.8999           26.31s\n",
      "        60           2.8712           22.65s\n",
      "        70           2.8495           19.70s\n",
      "        80           2.8280           17.50s\n",
      "        90           2.8173           15.55s\n",
      "       100           2.8069           13.90s\n",
      "       200           2.7738            3.39s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         481.3149            1.04m\n",
      "         2         266.5645            1.09m\n",
      "         3         157.4815            1.12m\n",
      "         4          97.4209            1.13m\n",
      "         5          60.1347            1.13m\n",
      "         6          39.2547            1.13m\n",
      "         7          26.1128            1.12m\n",
      "         8          17.6034            1.12m\n",
      "         9          12.5383            1.10m\n",
      "        10           9.1741            1.10m\n",
      "        20           3.1141           56.85s\n",
      "        30           2.9780           41.43s\n",
      "        40           2.9215           32.77s\n",
      "        50           2.8801           27.58s\n",
      "        60           2.8569           23.59s\n",
      "        70           2.8386           20.57s\n",
      "        80           2.8197           18.20s\n",
      "        90           2.8072           16.21s\n",
      "       100           2.7948           14.70s\n",
      "       200           2.7735            3.57s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         914.3983           22.34s\n",
      "         2         873.5831           21.38s\n",
      "         3         837.8045           21.01s\n",
      "         4         805.2334           20.94s\n",
      "         5         774.5404           20.81s\n",
      "         6         744.9678           20.60s\n",
      "         7         720.8102           20.43s\n",
      "         8         696.4548           20.36s\n",
      "         9         674.8329           20.23s\n",
      "        10         654.8210           20.22s\n",
      "        20         514.1905           19.26s\n",
      "        30         424.3866           18.64s\n",
      "        40         361.7860           17.80s\n",
      "        50         313.4078           17.08s\n",
      "        60         275.7072           16.30s\n",
      "        70         245.4969           15.54s\n",
      "        80         219.1129           14.81s\n",
      "        90         197.4946           14.06s\n",
      "       100         179.5734           13.32s\n",
      "       200          72.0192            6.52s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         915.5407           20.55s\n",
      "         2         875.7040           20.49s\n",
      "         3         839.1179           20.61s\n",
      "         4         807.3240           20.57s\n",
      "         5         775.8892           20.51s\n",
      "         6         748.2333           20.45s\n",
      "         7         723.5175           20.56s\n",
      "         8         700.6196           20.51s\n",
      "         9         679.3970           20.52s\n",
      "        10         658.7686           20.45s\n",
      "        20         510.7793           19.61s\n",
      "        30         420.9715           18.85s\n",
      "        40         360.0209           17.97s\n",
      "        50         315.6102           17.08s\n",
      "        60         277.4852           16.31s\n",
      "        70         248.3763           15.52s\n",
      "        80         223.1207           14.77s\n",
      "        90         202.7661           14.08s\n",
      "       100         184.2753           13.35s\n",
      "       200          74.9266            6.55s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         916.7109           20.85s\n",
      "         2         872.9782           20.93s\n",
      "         3         834.0914           20.81s\n",
      "         4         798.3190           20.72s\n",
      "         5         765.5502           20.69s\n",
      "         6         735.5102           20.70s\n",
      "         7         710.1638           20.56s\n",
      "         8         686.0427           20.69s\n",
      "         9         663.2345           20.61s\n",
      "        10         642.4670           20.54s\n",
      "        20         495.4198           19.51s\n",
      "        30         405.7717           18.68s\n",
      "        40         342.5307           17.82s\n",
      "        50         294.7370           17.13s\n",
      "        60         261.4793           16.31s\n",
      "        70         231.3114           15.53s\n",
      "        80         204.9409           14.79s\n",
      "        90         184.3549           14.05s\n",
      "       100         165.6837           13.35s\n",
      "       200          67.8306            6.52s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         894.0549           27.08s\n",
      "         2         838.2471           26.82s\n",
      "         3         791.4431           26.68s\n",
      "         4         748.0567           26.58s\n",
      "         5         713.0831           26.42s\n",
      "         6         678.3743           26.35s\n",
      "         7         649.9006           26.23s\n",
      "         8         622.4238           26.21s\n",
      "         9         599.5853           26.35s\n",
      "        10         578.6738           26.26s\n",
      "        20         428.0649           25.51s\n",
      "        30         339.5697           24.62s\n",
      "        40         280.0208           23.94s\n",
      "        50         235.2912           23.05s\n",
      "        60         203.4312           22.15s\n",
      "        70         175.5983           21.33s\n",
      "        80         153.7177           20.59s\n",
      "        90         133.8625           19.84s\n",
      "       100         115.3835           19.22s\n",
      "       200          32.1629           12.37s\n",
      "       300           9.9191            5.75s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         895.6145           26.31s\n",
      "         2         840.8631           26.62s\n",
      "         3         791.6884           26.68s\n",
      "         4         749.7195           26.68s\n",
      "         5         712.4371           26.88s\n",
      "         6         680.3369           27.30s\n",
      "         7         649.9492           27.26s\n",
      "         8         622.6053           27.30s\n",
      "         9         598.6265           27.19s\n",
      "        10         575.9185           27.13s\n",
      "        20         423.1420           26.05s\n",
      "        30         337.2890           24.87s\n",
      "        40         280.3131           23.79s\n",
      "        50         234.7881           22.95s\n",
      "        60         199.5638           22.18s\n",
      "        70         172.1179           21.39s\n",
      "        80         150.6318           20.63s\n",
      "        90         133.2889           19.86s\n",
      "       100         117.7743           19.13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       200          36.1706           12.32s\n",
      "       300          13.7458            5.74s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         895.7825           27.47s\n",
      "         2         836.6295           27.40s\n",
      "         3         784.5114           27.33s\n",
      "         4         739.4746           27.35s\n",
      "         5         700.9082           27.18s\n",
      "         6         667.1912           27.11s\n",
      "         7         636.1732           27.37s\n",
      "         8         609.2906           27.30s\n",
      "         9         583.9537           27.19s\n",
      "        10         563.4673           27.24s\n",
      "        20         407.4812           25.77s\n",
      "        30         315.7037           24.81s\n",
      "        40         260.2215           23.93s\n",
      "        50         219.3880           23.23s\n",
      "        60         186.0205           22.37s\n",
      "        70         159.3622           21.60s\n",
      "        80         135.8430           20.84s\n",
      "        90         118.7910           20.08s\n",
      "       100         103.5890           19.36s\n",
      "       200          32.0058           12.39s\n",
      "       300          11.7880            5.76s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         771.5022           52.15s\n",
      "         2         637.5065           53.77s\n",
      "         3         530.6865           53.90s\n",
      "         4         447.0098           54.49s\n",
      "         5         374.8962           54.49s\n",
      "         6         316.0419           54.64s\n",
      "         7         268.5593           55.17s\n",
      "         8         232.3070           55.54s\n",
      "         9         200.4061           55.44s\n",
      "        10         172.7221           55.41s\n",
      "        20          40.6552           53.80s\n",
      "        30          12.0416           51.86s\n",
      "        40           3.8665           49.33s\n",
      "        50           1.2597           46.78s\n",
      "        60           0.4420           43.83s\n",
      "        70           0.3045           38.20s\n",
      "        80           0.2846           32.38s\n",
      "        90           0.2846           27.38s\n",
      "       100           0.2846           23.38s\n",
      "       200           0.2846            5.26s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         758.5798           52.99s\n",
      "         2         607.3903           55.30s\n",
      "         3         497.5144           55.75s\n",
      "         4         412.5516           56.84s\n",
      "         5         343.4934           56.96s\n",
      "         6         289.3587           56.83s\n",
      "         7         239.8641           57.04s\n",
      "         8         201.4985           57.03s\n",
      "         9         170.6460           56.92s\n",
      "        10         144.3914           56.79s\n",
      "        20          38.7354           54.00s\n",
      "        30          13.6369           51.42s\n",
      "        40           6.6365           48.63s\n",
      "        50           4.3393           45.82s\n",
      "        60           3.4487           43.04s\n",
      "        70           3.1243           39.93s\n",
      "        80           3.0549           35.16s\n",
      "        90           3.0234           30.86s\n",
      "       100           2.9950           27.38s\n",
      "       200           2.8492            8.35s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         753.6959           53.27s\n",
      "         2         603.1543           55.58s\n",
      "         3         487.2785           56.40s\n",
      "         4         400.7699           56.56s\n",
      "         5         333.9720           56.79s\n",
      "         6         277.1959           56.83s\n",
      "         7         231.7847           56.84s\n",
      "         8         196.1737           56.59s\n",
      "         9         166.2138           56.41s\n",
      "        10         143.2149           56.25s\n",
      "        20          37.5229           53.54s\n",
      "        30          13.3508           51.03s\n",
      "        40           6.2327           48.50s\n",
      "        50           4.1715           45.76s\n",
      "        60           3.3333           43.05s\n",
      "        70           3.0918           39.35s\n",
      "        80           3.0396           34.60s\n",
      "        90           2.9984           30.75s\n",
      "       100           2.9652           27.59s\n",
      "       200           2.8283            8.73s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         701.3924           42.51s\n",
      "         2         564.6941           42.20s\n",
      "         3         464.5595           42.61s\n",
      "         4         380.9757           42.66s\n",
      "         5         326.6084           42.48s\n",
      "         6         277.6803           42.40s\n",
      "         7         239.9821           42.50s\n",
      "         8         209.6574           42.39s\n",
      "         9         185.2252           42.04s\n",
      "        10         166.1303           42.08s\n",
      "        20          60.0913           40.65s\n",
      "        30          26.8925           38.98s\n",
      "        40          12.7450           37.72s\n",
      "        50           6.3335           36.51s\n",
      "        60           3.2154           35.28s\n",
      "        70           1.5348           34.24s\n",
      "        80           0.7224           33.27s\n",
      "        90           0.3564           32.17s\n",
      "       100           0.2867           29.16s\n",
      "       200           0.2867           10.53s\n",
      "       300           0.2867            4.22s\n",
      "       400           0.2867            0.98s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         701.7826           42.07s\n",
      "         2         553.4079           42.64s\n",
      "         3         447.1543           43.06s\n",
      "         4         376.6255           42.88s\n",
      "         5         315.1868           42.83s\n",
      "         6         277.6568           42.47s\n",
      "         7         239.7647           42.69s\n",
      "         8         212.4610           42.44s\n",
      "         9         183.7843           42.32s\n",
      "        10         168.6844           41.91s\n",
      "        20          64.6627           40.54s\n",
      "        30          29.4083           39.05s\n",
      "        40          15.0605           38.25s\n",
      "        50           8.9483           36.84s\n",
      "        60           5.9731           35.71s\n",
      "        70           4.6257           34.60s\n",
      "        80           3.8592           33.45s\n",
      "        90           3.4338           32.27s\n",
      "       100           3.1758           31.17s\n",
      "       200           2.8940           16.47s\n",
      "       300           2.8199            8.43s\n",
      "       400           2.7940            2.36s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         689.7837           43.39s\n",
      "         2         530.8982           43.74s\n",
      "         3         429.6045           43.79s\n",
      "         4         351.4072           43.87s\n",
      "         5         288.7432           43.53s\n",
      "         6         251.3520           42.98s\n",
      "         7         213.2121           43.37s\n",
      "         8         192.0769           42.82s\n",
      "         9         168.1894           42.95s\n",
      "        10         152.7277           42.56s\n",
      "        20          55.9123           40.41s\n",
      "        30          26.8862           38.82s\n",
      "        40          14.3767           37.52s\n",
      "        50           8.6599           36.27s\n",
      "        60           5.8374           35.09s\n",
      "        70           4.3705           34.00s\n",
      "        80           3.6588           32.98s\n",
      "        90           3.2880           31.92s\n",
      "       100           3.0777           30.86s\n",
      "       200           2.8631           16.42s\n",
      "       300           2.7981            8.61s\n",
      "       400           2.7799            2.47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:  8.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1073.6940            1.07m\n",
      "         2         838.9251            1.06m\n",
      "         3         670.7632            1.06m\n",
      "         4         556.6949            1.06m\n",
      "         5         464.9471            1.06m\n",
      "         6         399.5829            1.06m\n",
      "         7         351.5738            1.06m\n",
      "         8         301.4647            1.06m\n",
      "         9         271.9807            1.05m\n",
      "        10         239.0724            1.05m\n",
      "        20          85.7936            1.00m\n",
      "        30          38.1030           57.58s\n",
      "        40          19.0495           55.29s\n",
      "        50          11.0590           53.18s\n",
      "        60           7.6833           50.91s\n",
      "        70           6.2398           48.56s\n",
      "        80           5.3610           46.37s\n",
      "        90           4.9729           44.03s\n",
      "       100           4.6669           41.85s\n",
      "       200           4.3030           18.80s\n",
      "       300           4.2087            4.33s\n"
     ]
    }
   ],
   "source": [
    "classifiers = wee.TrainBaselineClassifiers(X_train, y_train, n_iter=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SVM': 0.9902723735408561, 'GradientBoostingClassifier': 0.9980544747081712}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wee.TestBaselineClassifiers(X_train, y_train, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SVM': 0.8023255813953488, 'GradientBoostingClassifier': 0.7383720930232558}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wee.TestBaselineClassifiers(X_test, y_test, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8081395348837209"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmRSCV.best_estimator_.score(vectTexts_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "Base: Bidirectional Attention Flow for Machine Comprehension https://arxiv.org/abs/1611.01603\n",
    "\n",
    "TODO: Write data generator, model compatible with scikit-learn RandomSearchCV\n",
    "\n",
    "Inputs to the model:\n",
    "\n",
    "- Integer codes of each word\n",
    "- Integer codes of each character of each word\n",
    "- Pretrained embeddings for each word\n",
    "\n",
    "## Step 1: Encode characters\n",
    "\n",
    "For the character embedding layer, we will use all available characters plus the [PAD] character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "##############\n",
    "#Dictionaries\n",
    "##############\n",
    "\n",
    "#for all dicts, add 1 to result and reserve 0 to not found\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "\n",
    "allchars = string.printable\n",
    "allchars = [x for x in allchars] + ['PAD']\n",
    "allchars = { allchars[i]:i for i in range(len(allchars)) if allchars[i] not in [' ','\\n']}\n",
    "\n",
    "def extractVocabulary(textSet, maxWords = 3000):\n",
    "    #extracts vocabulary from a list of texts\n",
    "    #preprocessing to remove accents and uppercase should be done before\n",
    "    \n",
    "    countVec = CountVectorizer(max_features=maxWords, lowercase=False, strip_accents=None)\n",
    "    countVec.fit(textSet)\n",
    "    \n",
    "    #append punctuation\n",
    "    vocab = countVec.vocabulary_\n",
    "    n = len(vocab)\n",
    "    for x in string.punctuation:\n",
    "        vocab[x]=n\n",
    "        n += 1\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def sentence2code(sentence, vocabulary, embClass = None):\n",
    "    \"\"\"\n",
    "    Converts a sentence to char embedding codes, word embedding codes and embeddings\n",
    "    \n",
    "    vocabulary - dictionary that maps words to integers\n",
    "    sentence - list of words in the sentence, usually from preProcessing.clean_text().split(' ')\n",
    "    embClass - a class that implements method encodeWord and has property embDim (embedding dimension)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert type(sentence) == list, 'sentence should be a list of words'\n",
    "    \n",
    "    #sentence\n",
    "    sentCode = [vocabulary.get(w,-1)+1 for w in sentence]\n",
    "    \n",
    "    #characters\n",
    "    sent_len = len(sentence)\n",
    "    maxwlen = np.max([len(x) for x in sentence])\n",
    "    charCodes = np.zeros( (sent_len, maxwlen) ) + allchars['PAD']\n",
    "    \n",
    "    for i in range(sent_len):\n",
    "        charEnc = [allchars.get(cc, -1)+1 for cc in sentence[i]]\n",
    "        charCodes[i, 0:len(charEnc)] = charEnc\n",
    "    \n",
    "    wordEmbeddings = None\n",
    "    if embClass is not None:\n",
    "        wordEmbeddings = np.zeros ((sent_len, embClass.embDim))\n",
    "        for i in range(sent_len):\n",
    "            wordEmbeddings[i] = embClass.encodeWord(sentence[i])\n",
    "        \n",
    "    return np.array(sentCode), charCodes.astype(int), wordEmbeddings\n",
    "\n",
    "\n",
    "##############\n",
    "#Keras models\n",
    "##############\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#change LSTM to CuDNNLSTM\n",
    "from keras.layers import LSTM #CuDNNLSTM as LSTM\n",
    "from keras.layers import Input, Embedding, Conv2D, Lambda, Concatenate, Bidirectional, TimeDistributed, Dense\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "\n",
    "def createCharEncoder(charDictSize, embSize, nFiltersNGram=16, filterSize = 5):\n",
    "    \"\"\"\n",
    "    Creates a character encoder. Receives the integer code of the character.\n",
    "    \n",
    "    charDictSize - Length of dictionary of characters\n",
    "    embSize - Embedding size\n",
    "    \"\"\"\n",
    "    inp = Input((None, ))\n",
    "    \n",
    "    embedded = Embedding(charDictSize, embSize)(inp)\n",
    "        \n",
    "    embedded = Lambda(lambda x: K.expand_dims(x))(embedded)\n",
    "    ngram = Conv2D(nFiltersNGram, kernel_size = (5,1), padding='same', activation='relu')(embedded)\n",
    "    ngram = Conv2D(1, kernel_size = (filterSize,1), padding='same', activation=None)(ngram)\n",
    "    \n",
    "    ngram = Lambda(lambda x: K.squeeze(x, axis=3))(ngram)\n",
    "    ngram = Bidirectional(LSTM(embSize//2))(ngram)\n",
    "    \n",
    "    output = ngram\n",
    "    \n",
    "    model = Model(inputs=[inp], outputs=[output], name='CharEncoder')\n",
    "    return model\n",
    "\n",
    "def createDocEncoder(dictSize, embSize, nFiltersWordGram = 10, filterSize = 5, embDim = None):\n",
    "    \"\"\"\n",
    "    Creates a document encoder. Receives the integer code of the words.\n",
    "    \n",
    "    dictSize - Length of word dictionary\n",
    "    embSize - Embedding size\n",
    "    \"\"\"\n",
    "    inp = Input((None, ))\n",
    "    \n",
    "    embedded = Embedding(dictSize, embSize)(inp)\n",
    "    \n",
    "    #combine learned and pretrained embeddings\n",
    "    if embDim is not None:\n",
    "        preTrainedEmb = Input((None, embDim))\n",
    "        embedded = Concatenate()([embedded, preTrainedEmb])\n",
    "\n",
    "    \n",
    "    embedded = Lambda(lambda x: K.expand_dims(x))(embedded)\n",
    "    ngram = Conv2D(nFiltersWordGram, kernel_size = (filterSize,1), padding='same', activation='relu')(embedded)\n",
    "    ngram = Conv2D(1, kernel_size = (filterSize,1), padding='same', activation=None)(ngram)\n",
    "    \n",
    "    ngram = Lambda(lambda x: K.squeeze(x, axis=3))(ngram)\n",
    "    \n",
    "    output = ngram\n",
    "    \n",
    "    if embDim is None:\n",
    "        model = Model(inputs=[inp], outputs=[output], name='WordEncoder')\n",
    "    else:\n",
    "        model = Model(inputs=[inp, preTrainedEmb], outputs=[output], name='WordEncoderWithPreEmb')\n",
    "    return model  \n",
    "\n",
    "\n",
    "def createBiDirAttModel(charDictSize, dictSize,\n",
    "                        charEmbSize=16, nFiltersNGram=16, charfilterSize = 5, #character params\n",
    "                        wordEmbSize=128, nFiltersWordGram = 10, wordfilterSize = 5, preTrainedEmbDim = None,\n",
    "                        modelType = 'classifier', nClasses = 3): #word params\n",
    "    \n",
    "    inputChars = Input((None, None))\n",
    "    cFeatLayer = createCharEncoder(charDictSize, charEmbSize, nFiltersNGram, charfilterSize)\n",
    "    charFeats = TimeDistributed(cFeatLayer)(inputChars)\n",
    "    \n",
    "    inputWords = Input((None, ))\n",
    "    if preTrainedEmbDim is not None:\n",
    "        preTrainedEmb = Input((None, preTrainedEmbDim))\n",
    "        wordEncoded = createDocEncoder(dictSize, wordEmbSize, nFiltersWordGram, \n",
    "                                       wordfilterSize, preTrainedEmbDim)([inputWords, preTrainedEmb])\n",
    "    else:\n",
    "        wordEncoded = createDocEncoder(dictSize, wordEmbSize, nFiltersWordGram, \n",
    "                                       wordfilterSize, preTrainedEmbDim)(inputWords)\n",
    "        \n",
    "    output = Concatenate()([wordEncoded, charFeats])\n",
    "    output = Bidirectional(LSTM(wordEmbSize//2, return_sequences=True))(output)\n",
    "\n",
    "    if modelType == 'classifier':\n",
    "        output = Bidirectional(LSTM(wordEmbSize, return_sequences=False))(output)\n",
    "        output = Dense(wordEmbSize//2, activation = 'relu')(output)\n",
    "        output = Dense(wordEmbSize//4, activation = 'relu')(output)\n",
    "        output = Dense(nClasses, activation = 'softmax')(output)\n",
    "\n",
    "    \n",
    "    if preTrainedEmbDim is None:\n",
    "        model = Model(inputs=[inputWords, inputChars], outputs=[output], name='BiAttEnc')\n",
    "    else:\n",
    "        model = Model(inputs=[inputWords, inputChars, preTrainedEmb], outputs=[output], name='BiAttEncWithPretrainedEmb')\n",
    "        \n",
    "        \n",
    "        \n",
    "    return model\n",
    "\n",
    "##############################\n",
    "#Scikit learn compatible model\n",
    "##############################\n",
    "import math\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001                \n",
    "    drop = 0.6\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "            math.floor((1+epoch)/epochs_drop))\n",
    "    \n",
    "    if (lrate < 5e-6):\n",
    "        lrate = 5e-6\n",
    "      \n",
    "    print('Changing learning rate to {}'.format(lrate))\n",
    "    return lrate\n",
    "\n",
    "class BiDirAttModelClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, nClasses, charDictSize=len(allchars)+2, dictSize = 10000, preTrainedEmbeddings = None,\n",
    "                 modelFileName='model-text.h5',\n",
    "                 charEmbSize=16, nFiltersNGram=8, charfilterSize = 4, #character params\n",
    "                 wordEmbSize=128, nFiltersWordGram = 8, wordfilterSize = 4): #word params\n",
    "        \"\"\"\n",
    "        Initializes classifier\n",
    "        \n",
    "        charDictSize - size of character dictionary\n",
    "        dictSize - word dictionary size (for trainable embeddings)\n",
    "        \"\"\"\n",
    "        self.modelFileName = modelFileName\n",
    "        \n",
    "        self.charDictSize = charDictSize\n",
    "        self.dictSize = dictSize\n",
    "        self.charEmbSize=charEmbSize\n",
    "        self.nFiltersNGram=nFiltersNGram\n",
    "        self.charfilterSize=charfilterSize\n",
    "        self.wordEmbSize=wordEmbSize\n",
    "        self.nFiltersWordGram = nFiltersWordGram \n",
    "        self.wordfilterSize = wordfilterSize\n",
    "        \n",
    "        self.preTrainedEmbDim = None\n",
    "        self.preTrainedEmbeddings = None\n",
    "        if preTrainedEmbeddings is not None:\n",
    "            self.preTrainedEmbDim = preTrainedEmbeddings.embDim\n",
    "            self.preTrainedEmbeddings = preTrainedEmbeddings\n",
    "        \n",
    "        self.initialized = False\n",
    "    \n",
    "    def _initModel(self):\n",
    "        if self.initialized:\n",
    "            return\n",
    "        \n",
    "        self.model_ = createBiDirAttModel(self.charDictSize, self.dictSize,\n",
    "                                          charEmbSize=self.charEmbSize, nFiltersNGram=self.nFiltersNGram, \n",
    "                                          charfilterSize = self.charfilterSize, #character params\n",
    "                                          wordEmbSize=self.wordEmbSize, nFiltersWordGram = self.nFiltersWordGram, \n",
    "                                          wordfilterSize = self.wordfilterSize, preTrainedEmbDim = self.preTrainedEmbDim)\n",
    "        \n",
    "        #note that for sparse the target y has to have shape (batch_size, 1) <-the 1 matters\n",
    "        self.model_.compile(optimizer='adam', loss=['sparse_categorical_crossentropy'], metrics=['sparse_categorical_accuracy'])\n",
    "        \n",
    "        self.initialized = True\n",
    "    \n",
    "    def preprocess(self, X, y):\n",
    "        X_wCodes = []\n",
    "        X_cCodes = []\n",
    "        X_wEmbs = []\n",
    "        for s in X:\n",
    "            wCodes, cCodes, wEmbs = sentence2code(s.split(' '), self.vocab_, self.preTrainedEmbeddings)\n",
    "            X_wCodes.append(wCodes)\n",
    "            X_cCodes.append(cCodes)\n",
    "            X_wEmbs.append(wEmbs)\n",
    "        \n",
    "        #self.temp = X_wEmbs\n",
    "        \n",
    "        max_sentLen = max([cc.shape[0] for cc in X_cCodes])\n",
    "        max_wordLen = max([cc.shape[1] for cc in X_cCodes])\n",
    "        print('Maximum sentence length: {}. Maximum number of chars in a word: {}'.format(max_sentLen, max_wordLen))\n",
    "        \n",
    "        X_wCodes_transf = np.zeros( (len(X), max_sentLen), dtype=int )\n",
    "        X_cCodes_transf = np.zeros( (len(X), max_sentLen, max_wordLen), dtype=int )\n",
    "        if self.preTrainedEmbeddings is not None:\n",
    "            X_wEmbs_transf = np.zeros( (len(X), max_sentLen, self.preTrainedEmbeddings.embDim) )\n",
    "        else:\n",
    "            X_wEmbs_transf = None        \n",
    "            \n",
    "        for i in range(len(X)):\n",
    "            s_len = X_wCodes[i].shape[0]\n",
    "            w_len = X_cCodes[i].shape[1]\n",
    "            X_wCodes_transf[i, 0:s_len] =  X_wCodes[i]\n",
    "            X_cCodes_transf[i, 0:s_len, 0:w_len] = X_cCodes[i]\n",
    "            if self.preTrainedEmbeddings is not None:\n",
    "                X_wEmbs_transf[i, 0:X_wEmbs[i].shape[0], 0:X_wEmbs[i].shape[1]] = X_wEmbs[i]\n",
    "        \n",
    "        return X_wCodes_transf,X_cCodes_transf,X_wEmbs_transf, np.expand_dims(self.lblEncoder_.transform(y).astype(int),1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        assert len(X) == len(y), 'X and y must have the same length'\n",
    "\n",
    "        self.vocab_ = extractVocabulary(X, maxWords=self.dictSize)\n",
    "        self.dictSize = len(self.vocab_)+1\n",
    "        \n",
    "        if not self.initialized:\n",
    "            self._initModel()\n",
    "        \n",
    "        self.lblEncoder_ = preprocessing.LabelEncoder()\n",
    "        self.y_transf_train = self.lblEncoder_.fit(y)\n",
    "        \n",
    "        self.X_wCodes_train,self.X_cCodes_train,self.X_wEmbs_train, self.y_transf_train = self.preprocess(X,y)\n",
    "            \n",
    "        checkpointer = ModelCheckpoint(self.modelFileName, verbose=1, save_best_only=True, monitor='val_sparse_categorical_accuracy')    \n",
    "        lrate = LearningRateScheduler(step_decay)\n",
    "        earlystopper = EarlyStopping(patience=10, verbose=1, monitor='val_sparse_categorical_accuracy')\n",
    "        if self.preTrainedEmbeddings is not None:\n",
    "            self.results_ = self.model_.fit(x=[self.X_wCodes_train, self.X_cCodes_train, self.X_wEmbs_train], y=self.y_transf_train, epochs=35,\n",
    "                               verbose=1, validation_split=0.2, batch_size=128, callbacks=[checkpointer, earlystopper, lrate])\n",
    "        else:\n",
    "            self.results_ = self.model_.fit(x=[self.X_wCodes_train, self.X_cCodes_train], y=self.y_transf_train, epochs=35,\n",
    "                               verbose=1, validation_split=0.2, batch_size=128, callbacks=[checkpointer, earlystopper, lrate])\n",
    "            \n",
    "        # Check that X and y have correct shape\n",
    "        #X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        #self.classes_ = unique_labels(y)\n",
    "\n",
    "        #self.X_ = X\n",
    "        #self.y_ = y\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_wCodes_p,X_cCodes_p,X_wEmbs_p, y_transf_train = self.preprocess(X,[])\n",
    "        \n",
    "        y = self.model_.predict([X_wCodes_p,X_cCodes_p,X_wEmbs_p])\n",
    "        y = np.argmax(y, axis=1)\n",
    "        y = self.lblEncoder_.inverse_transform(y)\n",
    "        # Check is fit had been called\n",
    "        #check_is_fitted(self, ['X_', 'y_'])\n",
    "\n",
    "        # Input validation\n",
    "        #X = check_array(X)\n",
    "\n",
    "        #closest = np.argmin(euclidean_distances(X, self.X_), axis=1)\n",
    "        return y #self.y_[closest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdam = BiDirAttModelClassifier(3, wordEmbSize=64, preTrainedEmbeddings=wee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length: 49. Maximum number of chars in a word: 46\n",
      "Train on 1233 samples, validate on 309 samples\n",
      "Epoch 1/35\n",
      "Changing learning rate to 0.001\n",
      "1233/1233 [==============================] - ETA: 3:39 - loss: 1.0892 - sparse_categorical_accuracy: 0.554 - ETA: 1:54 - loss: 1.0794 - sparse_categorical_accuracy: 0.621 - ETA: 1:15 - loss: 1.0648 - sparse_categorical_accuracy: 0.648 - ETA: 54s - loss: 1.0510 - sparse_categorical_accuracy: 0.662 - ETA: 40s - loss: 1.0323 - sparse_categorical_accuracy: 0.67 - ETA: 28s - loss: 1.0193 - sparse_categorical_accuracy: 0.67 - ETA: 19s - loss: 1.0006 - sparse_categorical_accuracy: 0.68 - ETA: 11s - loss: 0.9974 - sparse_categorical_accuracy: 0.67 - ETA: 4s - loss: 0.9757 - sparse_categorical_accuracy: 0.6780 - 70s 56ms/step - loss: 0.9723 - sparse_categorical_accuracy: 0.6756 - val_loss: 0.9557 - val_sparse_categorical_accuracy: 0.6278\n",
      "\n",
      "Epoch 00001: val_sparse_categorical_accuracy improved from -inf to 0.62783, saving model to model-text.h5\n",
      "Epoch 2/35\n",
      "Changing learning rate to 0.001\n",
      "1233/1233 [==============================] - ETA: 39s - loss: 0.8026 - sparse_categorical_accuracy: 0.71 - ETA: 34s - loss: 0.7944 - sparse_categorical_accuracy: 0.71 - ETA: 29s - loss: 0.8263 - sparse_categorical_accuracy: 0.69 - ETA: 25s - loss: 0.8167 - sparse_categorical_accuracy: 0.69 - ETA: 20s - loss: 0.8130 - sparse_categorical_accuracy: 0.69 - ETA: 16s - loss: 0.8184 - sparse_categorical_accuracy: 0.69 - ETA: 11s - loss: 0.8117 - sparse_categorical_accuracy: 0.69 - ETA: 7s - loss: 0.8126 - sparse_categorical_accuracy: 0.6875 - ETA: 2s - loss: 0.8136 - sparse_categorical_accuracy: 0.690 - 48s 39ms/step - loss: 0.8104 - sparse_categorical_accuracy: 0.6959 - val_loss: 0.8416 - val_sparse_categorical_accuracy: 0.6278\n",
      "\n",
      "Epoch 00002: val_sparse_categorical_accuracy did not improve from 0.62783\n",
      "Epoch 3/35\n",
      "Changing learning rate to 0.001\n",
      "1233/1233 [==============================] - ETA: 39s - loss: 0.8327 - sparse_categorical_accuracy: 0.64 - ETA: 34s - loss: 0.8117 - sparse_categorical_accuracy: 0.66 - ETA: 30s - loss: 0.7994 - sparse_categorical_accuracy: 0.67 - ETA: 25s - loss: 0.7947 - sparse_categorical_accuracy: 0.67 - ETA: 21s - loss: 0.7892 - sparse_categorical_accuracy: 0.67 - ETA: 16s - loss: 0.7855 - sparse_categorical_accuracy: 0.68 - ETA: 11s - loss: 0.7846 - sparse_categorical_accuracy: 0.68 - ETA: 7s - loss: 0.7902 - sparse_categorical_accuracy: 0.6865 - ETA: 2s - loss: 0.7755 - sparse_categorical_accuracy: 0.696 - 48s 39ms/step - loss: 0.7792 - sparse_categorical_accuracy: 0.6959 - val_loss: 0.8686 - val_sparse_categorical_accuracy: 0.6278\n",
      "\n",
      "Epoch 00003: val_sparse_categorical_accuracy did not improve from 0.62783\n",
      "Epoch 4/35\n",
      "Changing learning rate to 0.001\n",
      "1233/1233 [==============================] - ETA: 38s - loss: 0.7913 - sparse_categorical_accuracy: 0.69 - ETA: 33s - loss: 0.7960 - sparse_categorical_accuracy: 0.67 - ETA: 29s - loss: 0.8075 - sparse_categorical_accuracy: 0.66 - ETA: 25s - loss: 0.8189 - sparse_categorical_accuracy: 0.66 - ETA: 20s - loss: 0.8010 - sparse_categorical_accuracy: 0.68 - ETA: 16s - loss: 0.7878 - sparse_categorical_accuracy: 0.69 - ETA: 11s - loss: 0.7904 - sparse_categorical_accuracy: 0.68 - ETA: 7s - loss: 0.7799 - sparse_categorical_accuracy: 0.6934 - ETA: 2s - loss: 0.7769 - sparse_categorical_accuracy: 0.695 - 48s 39ms/step - loss: 0.7739 - sparse_categorical_accuracy: 0.6959 - val_loss: 0.8426 - val_sparse_categorical_accuracy: 0.6278\n",
      "\n",
      "Epoch 00004: val_sparse_categorical_accuracy did not improve from 0.62783\n",
      "Epoch 5/35\n",
      "Changing learning rate to 0.001\n",
      "1233/1233 [==============================] - ETA: 39s - loss: 0.7417 - sparse_categorical_accuracy: 0.71 - ETA: 34s - loss: 0.7715 - sparse_categorical_accuracy: 0.68 - ETA: 30s - loss: 0.7450 - sparse_categorical_accuracy: 0.70 - ETA: 25s - loss: 0.7458 - sparse_categorical_accuracy: 0.69 - ETA: 21s - loss: 0.7592 - sparse_categorical_accuracy: 0.69 - ETA: 16s - loss: 0.7578 - sparse_categorical_accuracy: 0.69 - ETA: 11s - loss: 0.7541 - sparse_categorical_accuracy: 0.68 - ETA: 7s - loss: 0.7412 - sparse_categorical_accuracy: 0.6934 - ETA: 2s - loss: 0.7376 - sparse_categorical_accuracy: 0.694 - 48s 39ms/step - loss: 0.7312 - sparse_categorical_accuracy: 0.6959 - val_loss: 0.7936 - val_sparse_categorical_accuracy: 0.6278\n",
      "\n",
      "Epoch 00005: val_sparse_categorical_accuracy did not improve from 0.62783\n",
      "Epoch 6/35\n",
      "Changing learning rate to 0.001\n",
      "1233/1233 [==============================] - ETA: 39s - loss: 0.7422 - sparse_categorical_accuracy: 0.64 - ETA: 34s - loss: 0.6802 - sparse_categorical_accuracy: 0.66 - ETA: 30s - loss: 0.6455 - sparse_categorical_accuracy: 0.69 - ETA: 25s - loss: 0.6308 - sparse_categorical_accuracy: 0.71 - ETA: 21s - loss: 0.6455 - sparse_categorical_accuracy: 0.71 - ETA: 16s - loss: 0.6420 - sparse_categorical_accuracy: 0.71 - ETA: 12s - loss: 0.6479 - sparse_categorical_accuracy: 0.71 - ETA: 7s - loss: 0.6351 - sparse_categorical_accuracy: 0.7227 - ETA: 2s - loss: 0.6360 - sparse_categorical_accuracy: 0.719 - 49s 40ms/step - loss: 0.6216 - sparse_categorical_accuracy: 0.7291 - val_loss: 0.7683 - val_sparse_categorical_accuracy: 0.6893\n",
      "\n",
      "Epoch 00006: val_sparse_categorical_accuracy improved from 0.62783 to 0.68932, saving model to model-text.h5\n",
      "Epoch 7/35\n",
      "Changing learning rate to 0.001\n",
      "1233/1233 [==============================] - ETA: 40s - loss: 0.5760 - sparse_categorical_accuracy: 0.77 - ETA: 35s - loss: 0.5392 - sparse_categorical_accuracy: 0.78 - ETA: 30s - loss: 0.5420 - sparse_categorical_accuracy: 0.78 - ETA: 25s - loss: 0.5216 - sparse_categorical_accuracy: 0.79 - ETA: 21s - loss: 0.5194 - sparse_categorical_accuracy: 0.80 - ETA: 16s - loss: 0.5129 - sparse_categorical_accuracy: 0.81 - ETA: 11s - loss: 0.5058 - sparse_categorical_accuracy: 0.81 - ETA: 7s - loss: 0.5075 - sparse_categorical_accuracy: 0.8115 - ETA: 2s - loss: 0.5059 - sparse_categorical_accuracy: 0.812 - 48s 39ms/step - loss: 0.5071 - sparse_categorical_accuracy: 0.8110 - val_loss: 0.7317 - val_sparse_categorical_accuracy: 0.7055\n",
      "\n",
      "Epoch 00007: val_sparse_categorical_accuracy improved from 0.68932 to 0.70550, saving model to model-text.h5\n",
      "Epoch 8/35\n",
      "Changing learning rate to 0.001\n",
      "1233/1233 [==============================] - ETA: 38s - loss: 0.4340 - sparse_categorical_accuracy: 0.84 - ETA: 34s - loss: 0.5073 - sparse_categorical_accuracy: 0.81 - ETA: 30s - loss: 0.4430 - sparse_categorical_accuracy: 0.84 - ETA: 25s - loss: 0.4511 - sparse_categorical_accuracy: 0.83 - ETA: 21s - loss: 0.4335 - sparse_categorical_accuracy: 0.84 - ETA: 16s - loss: 0.4196 - sparse_categorical_accuracy: 0.84 - ETA: 11s - loss: 0.3936 - sparse_categorical_accuracy: 0.85 - ETA: 7s - loss: 0.4055 - sparse_categorical_accuracy: 0.8555 - ETA: 2s - loss: 0.4058 - sparse_categorical_accuracy: 0.853 - 48s 39ms/step - loss: 0.3984 - sparse_categorical_accuracy: 0.8573 - val_loss: 0.8119 - val_sparse_categorical_accuracy: 0.7346\n",
      "\n",
      "Epoch 00008: val_sparse_categorical_accuracy improved from 0.70550 to 0.73463, saving model to model-text.h5\n",
      "Epoch 9/35\n",
      "Changing learning rate to 0.001\n",
      "1233/1233 [==============================] - ETA: 39s - loss: 0.3180 - sparse_categorical_accuracy: 0.87 - ETA: 34s - loss: 0.3077 - sparse_categorical_accuracy: 0.88 - ETA: 30s - loss: 0.3242 - sparse_categorical_accuracy: 0.88 - ETA: 25s - loss: 0.3045 - sparse_categorical_accuracy: 0.89 - ETA: 21s - loss: 0.3157 - sparse_categorical_accuracy: 0.88 - ETA: 16s - loss: 0.3248 - sparse_categorical_accuracy: 0.87 - ETA: 12s - loss: 0.3355 - sparse_categorical_accuracy: 0.86 - ETA: 7s - loss: 0.3230 - sparse_categorical_accuracy: 0.8740 - ETA: 2s - loss: 0.3241 - sparse_categorical_accuracy: 0.868 - 48s 39ms/step - loss: 0.3214 - sparse_categorical_accuracy: 0.8702 - val_loss: 0.8817 - val_sparse_categorical_accuracy: 0.7540\n",
      "\n",
      "Epoch 00009: val_sparse_categorical_accuracy improved from 0.73463 to 0.75405, saving model to model-text.h5\n",
      "Epoch 10/35\n",
      "Changing learning rate to 0.0006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1233 [==============================] - ETA: 38s - loss: 0.3041 - sparse_categorical_accuracy: 0.88 - ETA: 33s - loss: 0.2899 - sparse_categorical_accuracy: 0.87 - ETA: 29s - loss: 0.2807 - sparse_categorical_accuracy: 0.88 - ETA: 24s - loss: 0.2921 - sparse_categorical_accuracy: 0.88 - ETA: 20s - loss: 0.2985 - sparse_categorical_accuracy: 0.87 - ETA: 16s - loss: 0.3050 - sparse_categorical_accuracy: 0.87 - ETA: 11s - loss: 0.3050 - sparse_categorical_accuracy: 0.87 - ETA: 7s - loss: 0.3036 - sparse_categorical_accuracy: 0.8809 - ETA: 2s - loss: 0.3000 - sparse_categorical_accuracy: 0.881 - 47s 38ms/step - loss: 0.2932 - sparse_categorical_accuracy: 0.8840 - val_loss: 0.8400 - val_sparse_categorical_accuracy: 0.7508\n",
      "\n",
      "Epoch 00010: val_sparse_categorical_accuracy did not improve from 0.75405\n",
      "Epoch 11/35\n",
      "Changing learning rate to 0.0006\n",
      "1233/1233 [==============================] - ETA: 38s - loss: 0.2385 - sparse_categorical_accuracy: 0.91 - ETA: 33s - loss: 0.2809 - sparse_categorical_accuracy: 0.88 - ETA: 29s - loss: 0.2884 - sparse_categorical_accuracy: 0.88 - ETA: 24s - loss: 0.2607 - sparse_categorical_accuracy: 0.90 - ETA: 20s - loss: 0.2595 - sparse_categorical_accuracy: 0.89 - ETA: 15s - loss: 0.2528 - sparse_categorical_accuracy: 0.89 - ETA: 11s - loss: 0.2467 - sparse_categorical_accuracy: 0.90 - ETA: 7s - loss: 0.2421 - sparse_categorical_accuracy: 0.9033 - ETA: 2s - loss: 0.2431 - sparse_categorical_accuracy: 0.901 - 47s 38ms/step - loss: 0.2392 - sparse_categorical_accuracy: 0.9027 - val_loss: 0.8392 - val_sparse_categorical_accuracy: 0.7379\n",
      "\n",
      "Epoch 00011: val_sparse_categorical_accuracy did not improve from 0.75405\n",
      "Epoch 12/35\n",
      "Changing learning rate to 0.0006\n",
      "1233/1233 [==============================] - ETA: 38s - loss: 0.2434 - sparse_categorical_accuracy: 0.89 - ETA: 34s - loss: 0.2369 - sparse_categorical_accuracy: 0.90 - ETA: 29s - loss: 0.2388 - sparse_categorical_accuracy: 0.90 - ETA: 25s - loss: 0.2335 - sparse_categorical_accuracy: 0.91 - ETA: 20s - loss: 0.2268 - sparse_categorical_accuracy: 0.91 - ETA: 16s - loss: 0.2237 - sparse_categorical_accuracy: 0.91 - ETA: 11s - loss: 0.2204 - sparse_categorical_accuracy: 0.91 - ETA: 7s - loss: 0.2128 - sparse_categorical_accuracy: 0.9180 - ETA: 2s - loss: 0.2113 - sparse_categorical_accuracy: 0.918 - 48s 39ms/step - loss: 0.2073 - sparse_categorical_accuracy: 0.9205 - val_loss: 0.9352 - val_sparse_categorical_accuracy: 0.7443\n",
      "\n",
      "Epoch 00012: val_sparse_categorical_accuracy did not improve from 0.75405\n",
      "Epoch 13/35\n",
      "Changing learning rate to 0.0006\n",
      "1233/1233 [==============================] - ETA: 38s - loss: 0.2392 - sparse_categorical_accuracy: 0.91 - ETA: 35s - loss: 0.1848 - sparse_categorical_accuracy: 0.92 - ETA: 30s - loss: 0.1789 - sparse_categorical_accuracy: 0.92 - ETA: 26s - loss: 0.1738 - sparse_categorical_accuracy: 0.92 - ETA: 21s - loss: 0.1863 - sparse_categorical_accuracy: 0.92 - ETA: 16s - loss: 0.1822 - sparse_categorical_accuracy: 0.92 - ETA: 12s - loss: 0.1837 - sparse_categorical_accuracy: 0.92 - ETA: 7s - loss: 0.1782 - sparse_categorical_accuracy: 0.9238 - ETA: 2s - loss: 0.1794 - sparse_categorical_accuracy: 0.922 - 49s 40ms/step - loss: 0.1772 - sparse_categorical_accuracy: 0.9246 - val_loss: 0.9711 - val_sparse_categorical_accuracy: 0.7379\n",
      "\n",
      "Epoch 00013: val_sparse_categorical_accuracy did not improve from 0.75405\n",
      "Epoch 14/35\n",
      "Changing learning rate to 0.0006\n",
      "1233/1233 [==============================] - ETA: 39s - loss: 0.1570 - sparse_categorical_accuracy: 0.92 - ETA: 34s - loss: 0.1605 - sparse_categorical_accuracy: 0.91 - ETA: 30s - loss: 0.1666 - sparse_categorical_accuracy: 0.92 - ETA: 25s - loss: 0.1736 - sparse_categorical_accuracy: 0.91 - ETA: 21s - loss: 0.1765 - sparse_categorical_accuracy: 0.91 - ETA: 16s - loss: 0.1681 - sparse_categorical_accuracy: 0.92 - ETA: 12s - loss: 0.1719 - sparse_categorical_accuracy: 0.91 - ETA: 7s - loss: 0.1618 - sparse_categorical_accuracy: 0.9258 - ETA: 2s - loss: 0.1590 - sparse_categorical_accuracy: 0.929 - 49s 40ms/step - loss: 0.1569 - sparse_categorical_accuracy: 0.9327 - val_loss: 1.1279 - val_sparse_categorical_accuracy: 0.7476\n",
      "\n",
      "Epoch 00014: val_sparse_categorical_accuracy did not improve from 0.75405\n",
      "Epoch 15/35\n",
      "Changing learning rate to 0.0006\n",
      "1233/1233 [==============================] - ETA: 39s - loss: 0.1372 - sparse_categorical_accuracy: 0.93 - ETA: 35s - loss: 0.1769 - sparse_categorical_accuracy: 0.91 - ETA: 30s - loss: 0.1563 - sparse_categorical_accuracy: 0.92 - ETA: 26s - loss: 0.1538 - sparse_categorical_accuracy: 0.92 - ETA: 21s - loss: 0.1421 - sparse_categorical_accuracy: 0.93 - ETA: 16s - loss: 0.1408 - sparse_categorical_accuracy: 0.93 - ETA: 12s - loss: 0.1452 - sparse_categorical_accuracy: 0.93 - ETA: 7s - loss: 0.1421 - sparse_categorical_accuracy: 0.9375 - ETA: 2s - loss: 0.1403 - sparse_categorical_accuracy: 0.936 - 49s 40ms/step - loss: 0.1361 - sparse_categorical_accuracy: 0.9408 - val_loss: 1.2028 - val_sparse_categorical_accuracy: 0.7346\n",
      "\n",
      "Epoch 00015: val_sparse_categorical_accuracy did not improve from 0.75405\n",
      "Epoch 16/35\n",
      "Changing learning rate to 0.0006\n",
      "1233/1233 [==============================] - ETA: 39s - loss: 0.1498 - sparse_categorical_accuracy: 0.95 - ETA: 35s - loss: 0.1277 - sparse_categorical_accuracy: 0.95 - ETA: 30s - loss: 0.1109 - sparse_categorical_accuracy: 0.95 - ETA: 25s - loss: 0.1100 - sparse_categorical_accuracy: 0.95 - ETA: 21s - loss: 0.1145 - sparse_categorical_accuracy: 0.95 - ETA: 16s - loss: 0.1191 - sparse_categorical_accuracy: 0.95 - ETA: 12s - loss: 0.1160 - sparse_categorical_accuracy: 0.95 - ETA: 7s - loss: 0.1134 - sparse_categorical_accuracy: 0.9600 - ETA: 2s - loss: 0.1101 - sparse_categorical_accuracy: 0.960 - 49s 40ms/step - loss: 0.1063 - sparse_categorical_accuracy: 0.9619 - val_loss: 1.2576 - val_sparse_categorical_accuracy: 0.7379\n",
      "\n",
      "Epoch 00016: val_sparse_categorical_accuracy did not improve from 0.75405\n",
      "Epoch 17/35\n",
      "Changing learning rate to 0.0006\n",
      "1233/1233 [==============================] - ETA: 41s - loss: 0.1204 - sparse_categorical_accuracy: 0.91 - ETA: 35s - loss: 0.1003 - sparse_categorical_accuracy: 0.92 - ETA: 30s - loss: 0.0874 - sparse_categorical_accuracy: 0.94 - ETA: 25s - loss: 0.0844 - sparse_categorical_accuracy: 0.95 - ETA: 21s - loss: 0.0859 - sparse_categorical_accuracy: 0.96 - ETA: 16s - loss: 0.0856 - sparse_categorical_accuracy: 0.96 - ETA: 12s - loss: 0.0820 - sparse_categorical_accuracy: 0.96 - ETA: 7s - loss: 0.0796 - sparse_categorical_accuracy: 0.9668 - ETA: 2s - loss: 0.0851 - sparse_categorical_accuracy: 0.962 - 49s 40ms/step - loss: 0.0824 - sparse_categorical_accuracy: 0.9651 - val_loss: 1.3125 - val_sparse_categorical_accuracy: 0.7443\n",
      "\n",
      "Epoch 00017: val_sparse_categorical_accuracy did not improve from 0.75405\n",
      "Epoch 18/35\n",
      "Changing learning rate to 0.0006\n",
      "1233/1233 [==============================] - ETA: 40s - loss: 0.0658 - sparse_categorical_accuracy: 0.99 - ETA: 35s - loss: 0.0697 - sparse_categorical_accuracy: 0.98 - ETA: 30s - loss: 0.0722 - sparse_categorical_accuracy: 0.97 - ETA: 25s - loss: 0.0793 - sparse_categorical_accuracy: 0.97 - ETA: 21s - loss: 0.0741 - sparse_categorical_accuracy: 0.97 - ETA: 16s - loss: 0.0711 - sparse_categorical_accuracy: 0.97 - ETA: 12s - loss: 0.0725 - sparse_categorical_accuracy: 0.97 - ETA: 7s - loss: 0.0699 - sparse_categorical_accuracy: 0.9775 - ETA: 2s - loss: 0.0682 - sparse_categorical_accuracy: 0.979 - 49s 39ms/step - loss: 0.0676 - sparse_categorical_accuracy: 0.9797 - val_loss: 1.4942 - val_sparse_categorical_accuracy: 0.7411\n",
      "\n",
      "Epoch 00018: val_sparse_categorical_accuracy did not improve from 0.75405\n",
      "Epoch 19/35\n",
      "Changing learning rate to 0.0006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233/1233 [==============================] - ETA: 40s - loss: 0.0497 - sparse_categorical_accuracy: 0.97 - ETA: 35s - loss: 0.0416 - sparse_categorical_accuracy: 0.98 - ETA: 30s - loss: 0.0450 - sparse_categorical_accuracy: 0.98 - ETA: 26s - loss: 0.0545 - sparse_categorical_accuracy: 0.98 - ETA: 21s - loss: 0.0640 - sparse_categorical_accuracy: 0.97 - ETA: 16s - loss: 0.0597 - sparse_categorical_accuracy: 0.98 - ETA: 12s - loss: 0.0634 - sparse_categorical_accuracy: 0.97 - ETA: 7s - loss: 0.0609 - sparse_categorical_accuracy: 0.9756 - ETA: 2s - loss: 0.0567 - sparse_categorical_accuracy: 0.978 - 49s 40ms/step - loss: 0.0592 - sparse_categorical_accuracy: 0.9781 - val_loss: 1.4460 - val_sparse_categorical_accuracy: 0.7184\n",
      "\n",
      "Epoch 00019: val_sparse_categorical_accuracy did not improve from 0.75405\n",
      "Epoch 00019: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BiDirAttModelClassifier(charDictSize=101, charEmbSize=16, charfilterSize=4,\n",
       "            dictSize=4741, modelFileName='model-text.h5', nClasses=None,\n",
       "            nFiltersNGram=8, nFiltersWordGram=8,\n",
       "            preTrainedEmbeddings=<Embeddings.WordEmbeddingBR object at 0x0000013B6F587588>,\n",
       "            wordEmbSize=64, wordfilterSize=4)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdam.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length: 47. Maximum number of chars in a word: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Douglas\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7674418604651163"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(bdam.predict(X_test)==np.array(y_test))/len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "results = bdam.results_\n",
    "# summarize history for intersestion over union\n",
    "plt.plot(results.history['sparse_categorical_accuracy'])\n",
    "plt.plot(results.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('sparse_categorical_accuracy')\n",
    "plt.ylabel('sparse_categorical_accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1542, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = bdam.model_.predict([bdam.X_wCodes_train, bdam.X_cCodes_train, bdam.X_wEmbs_train], verbose=1)\n",
    "ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32663018, 0.3347336 , 0.33863625], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(bdam.X_cCodes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length: 49. Maximum number of chars in a word: 46\n",
      "Train on 1387 samples, validate on 155 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[74,1] = 100 is not in [0, 100)\n\t [[Node: time_distributed_1/embedding_1/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training/Adam/Assign_17\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_1/embeddings/read, time_distributed_1/embedding_1/Cast, training/Adam/gradients/time_distributed_1/embedding_1/embedding_lookup_grad/concat/axis)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-294996380317>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbdam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-981bcddaf10c>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreTrainedEmbeddings\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             self.model_.fit(x=[self.X_wCodes_train, self.X_cCodes_train, self.X_wEmbs_train], y=self.y_transf_train, epochs=5,\n\u001b[1;32m--> 262\u001b[1;33m                            verbose=1, validation_split=0.1)\n\u001b[0m\u001b[0;32m    263\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m             self.model_.fit(x=[self.X_wCodes_train, self.X_cCodes_train], y=self.y_transf_train, epochs=5,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[1;32m-> 1454\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[74,1] = 100 is not in [0, 100)\n\t [[Node: time_distributed_1/embedding_1/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training/Adam/Assign_17\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_1/embeddings/read, time_distributed_1/embedding_1/Cast, training/Adam/gradients/time_distributed_1/embedding_1/embedding_lookup_grad/concat/axis)]]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       ...,\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdam.y_transf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'achei a comida bem medíocre . prato com muitas coisas , mas nada com sabor . não vale o que custa .'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Douglas\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.,  0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = bdam.lblEncoder_.transform([-1,1,0]).astype(int)\n",
    "print(tt)\n",
    "bdam.lblEncoder_.inverse_transform(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_42 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_40 (InputLayer)           (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "WordEncoder (Model)             (None, None, 128)    1280111     input_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, None, 2)      525         input_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, None, 130)    0           WordEncoder[1][0]                \n",
      "                                                                 time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, None, 128)    100352      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_22 (Bidirectional (None, 256)          264192      bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           16448       bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           2080        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 3)            99          dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,663,807\n",
      "Trainable params: 1,663,807\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bdam.model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bons cortes grelhados , especialmente o galeto . buffet de saladas bom , com destaque para os legumes grelhados . atendimento rápido e pratos idem . preço mais caro , mas vale pela comida deliciosa .'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = extractVocabulary(X_train, maxWords=10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[8].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.233608,  0.165974, -0.051004, ..., -0.107206,  0.082516,\n",
       "         0.088683],\n",
       "       [-0.094895,  0.101375, -0.111975, ..., -0.021888, -0.060563,\n",
       "         0.145541],\n",
       "       [ 0.087262,  0.117833, -0.032012, ..., -0.292878, -0.085171,\n",
       "        -0.281145],\n",
       "       ...,\n",
       "       [ 0.291259,  0.107393,  0.136992, ..., -0.208725,  0.023768,\n",
       "         0.128399],\n",
       "       [ 0.223499, -0.257117,  0.134983, ...,  0.073602,  0.179686,\n",
       "        -0.071289],\n",
       "       [-0.232425, -0.06775 , -0.238036, ...,  0.046183,  0.13444 ,\n",
       "        -0.125707]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2code(X_train[8].split(' '), vocab, embClass=wee)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, None, 200)    800000      input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, None, 50)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 250)    0           embedding_6[0][0]                \n",
      "                                                                 input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, None, 250, 1) 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, 250, 10 60          lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, 250, 1) 51          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, None, 250)    0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, None, 200)    281600      lambda_12[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,081,711\n",
      "Trainable params: 1,081,711\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m=createDocEncoder(4000,200, embDim=50)\n",
    "m.summary()\n",
    "#plot_model(m, to_file='wordEncoder.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, None, 16)          1600      \n",
      "_________________________________________________________________\n",
      "lambda_13 (Lambda)           (None, None, 16, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, None, 16, 16)      96        \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, None, 16, 1)       81        \n",
      "_________________________________________________________________\n",
      "lambda_14 (Lambda)           (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 32)                4352      \n",
      "=================================================================\n",
      "Total params: 6,129\n",
      "Trainable params: 6,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = createCharEncoder(100, 16)\n",
    "m.summary()\n",
    "#plot_model(m, to_file='characterEncoder.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"input_20:0\", shape=(?, ?, ?), dtype=float32) at layer \"input_20\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-c20c424b3d4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateBiDirAttModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-c778399bf433>\u001b[0m in \u001b[0;36mcreateBiDirAttModel\u001b[1;34m(charDictSize, dictSize, charEmbSize, nFiltersNGram, charfilterSize, wordEmbSize, nFiltersWordGram, wordfilterSize, preTrainedEmbDim)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwordEncoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharFeats\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpreTrainedEmbDim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputWords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'BiAttEnc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputWords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputChars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreTrainedEmb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'BiAttEncWithPretrainedEmb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[0;32m     92\u001b[0m             \u001b[1;31m# Graph network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;31m# Subclassed network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[1;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[1;32m--> 237\u001b[1;33m             self.inputs, self.outputs)\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m                                          \u001b[1;34m'The following previous layers '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m                                          \u001b[1;34m'were accessed without issue: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m                                          str(layers_with_complete_input))\n\u001b[0m\u001b[0;32m   1431\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m                     \u001b[0mcomputable_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_20:0\", shape=(?, ?, ?), dtype=float32) at layer \"input_20\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "m = createBiDirAttModel(16, 3000, preTrainedEmbDim=50)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
